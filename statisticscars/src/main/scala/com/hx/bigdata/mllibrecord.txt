1、随机梯度下降

首先介绍一下随机梯度下降算法：
1.1、代码一：

package mllib


import org.apache.log4j.{Level, Logger}
import org.apache.spark.{SparkContext, SparkConf}

import scala.collection.mutable.HashMap

/**
  * 随机梯度下降算法
  * Created by 汪本成 on 2016/8/7.
  */
object SGD {

  //屏蔽不必要的日志显示在终端上
  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getLogger("org.apache.eclipse.jetty.server").setLevel(Level.OFF)

  //程序入口
  val conf = new SparkConf()
    .setMaster("local[1]")
    .setAppName(this.getClass().getSimpleName()
    .filter(!_.equals('$')))

  println(this.getClass().getSimpleName().filter(!_.equals('$')))

  val sc = new SparkContext(conf)

  //创建存储数据集HashMap集合
  val data = new HashMap[Int, Int]()
  //生成数据集内容
  def getData(): HashMap[Int, Int] = {
    for(i <- 1 to 50) {
      data += (i -> (2 * i))  //写入公式y=2x
    }
    data
  }

  //假设a=0
  var a: Double = 0
  //设置步进系数
  var b: Double = 0.1

  //设置迭代公式
  def sgd(x: Double, y: Double) = {
    a = a - b * ((a * x) - y)
  }

  def main(args: Array[String]) {
    //获取数据集
    val dataSource = getData()
    println("data: ")
    dataSource.foreach(each => println(each + " "))
    println("\nresult: ")
    var num = 1
    //开始迭代
    dataSource.foreach(myMap => {
      println(num + ":" + a + "("+myMap._1+","+myMap._2+")")
      sgd(myMap._1, myMap._2)
      num = num + 1
    })
    //显示结果
    println("最终结果a为 " + a)
  }

}

结果请大家自己验证。

2、线性回归
2.1、数据

首先是做下小数据集的实验，测试的公式在代码中有说明，实验数据如下：

[plain] view plain copy

    5,1 1
    7,2 1
    10,2 2
    9,3 2
    11,4 1
    19,5 3
    18,6 2

2.2、代码二：

package mllib

import org.apache.log4j.{Level, Logger}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LinearRegressionWithSGD, LabeledPoint}
import org.apache.spark.{SparkContext, SparkConf}

/**
  * 线性回归1-小数据集
  * 公式：f(x) = ax1 + bx2
  * Created by 汪本成 on 2016/8/6.
  */
object LinearRegression1 {

  //屏蔽不必要的日志显示在终端上
  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getLogger("org.apache.eclipse.jetty.server").setLevel(Level.OFF)

  //程序入口
  val conf = new SparkConf()
    .setMaster("local[1]")
    .setAppName(this.getClass().getSimpleName().filter(!_.equals('$')))
  println(this.getClass().getSimpleName().filter(!_.equals('$')))

  val sc = new SparkContext(conf)

  def main(args: Array[String]) {
    //获取数据集路径
    val data = sc.textFile("G:\\MLlibData\\lpsa2.txt")
    //处理数据集
    val parsedData = data.map { line =>
      val parts = line.split(',')
      //转化数据格式
      LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
    }.cache()
    //建立模型
    val numiteartor = 100
    val stepSize = 0.1
    val model = LinearRegressionWithSGD.train(parsedData, numiteartor, stepSize)
    //通过模型预测模型
    val result = model.predict(Vectors.dense(2, 1))
    println("model weights:")
    //计算两个系数，并以向量形式保存
    println(model.weights)
    println(result)
    sc.stop()
  }

}


3、回归曲线

回归曲线这块我们不仅预测结果和真实结果，还要计算回归曲线的MSE。
3.1、数据

[plain] view plain copy

    -0.4307829,-1.63735562648104 -2.00621178480549 -1.86242597251066 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    -0.1625189,-1.98898046126935 -0.722008756122123 -0.787896192088153 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    -0.1625189,-1.57881887548545 -2.1887840293994 1.36116336875686 -1.02470580167082 -0.522940888712441 -0.863171185425945 0.342627053981254 -0.155348103855541
    -0.1625189,-2.16691708463163 -0.807993896938655 -0.787896192088153 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    0.3715636,-0.507874475300631 -0.458834049396776 -0.250631301876899 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    0.7654678,-2.03612849966376 -0.933954647105133 -1.86242597251066 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    0.8544153,-0.557312518810673 -0.208756571683607 -0.787896192088153 0.990146852537193 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.2669476,-0.929360463147704 -0.0578991819441687 0.152317365781542 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.2669476,-2.28833047634983 -0.0706369432557794 -0.116315079324086 0.80409888772376 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.2669476,0.223498042876113 -1.41471935455355 -0.116315079324086 -1.02470580167082 -0.522940888712441 -0.29928234305568 0.342627053981254 0.199211097885341
    1.3480731,0.107785900236813 -1.47221551299731 0.420949810887169 -1.02470580167082 -0.522940888712441 -0.863171185425945 0.342627053981254 -0.687186906466865
    1.446919,0.162180092313795 -1.32557369901905 0.286633588334355 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.4701758,-1.49795329918548 -0.263601072284232 0.823898478545609 0.788388310173035 -0.522940888712441 -0.29928234305568 0.342627053981254 0.199211097885341
    1.4929041,0.796247055396743 0.0476559407005752 0.286633588334355 -1.02470580167082 -0.522940888712441 0.394013435896129 -1.04215728919298 -0.864466507337306
    1.5581446,-1.62233848461465 -0.843294091975396 -3.07127197548598 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.5993876,-0.990720665490831 0.458513517212311 0.823898478545609 1.07379746308195 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.6389967,-0.171901281967138 -0.489197399065355 -0.65357996953534 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.6956156,-1.60758252338831 -0.590700340358265 -0.65357996953534 -0.619561070667254 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    1.7137979,0.366273918511144 -0.414014962912583 -0.116315079324086 0.232904453212813 -0.522940888712441 0.971228997418125 0.342627053981254 1.26288870310799
    1.8000583,-0.710307384579833 0.211731938156277 0.152317365781542 -1.02470580167082 -0.522940888712441 -0.442797990776478 0.342627053981254 1.61744790484887
    1.8484548,-0.262791728113881 -1.16708345615721 0.420949810887169 0.0846342590816532 -0.522940888712441 0.163172393491611 0.342627053981254 1.97200710658975
    1.8946169,0.899043117369237 -0.590700340358265 0.152317365781542 -1.02470580167082 -0.522940888712441 1.28643254437683 -1.04215728919298 -0.864466507337306
    1.9242487,-0.903451690500615 1.07659722048274 0.152317365781542 1.28380453408541 -0.522940888712441 -0.442797990776478 -1.04215728919298 -0.864466507337306
    2.008214,-0.0633337899773081 -1.38088970920094 0.958214701098423 0.80409888772376 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    2.0476928,-1.15393789990757 -0.961853075398404 -0.116315079324086 -1.02470580167082 -0.522940888712441 -0.442797990776478 -1.04215728919298 -0.864466507337306
    2.1575593,0.0620203721138446 0.0657973885499142 1.22684714620405 -0.468824786336838 -0.522940888712441 1.31421001659859 1.72741139715549 -0.332627704725983
    2.1916535,-0.75731027755674 -2.92717970468456 0.018001143228728 -1.02470580167082 -0.522940888712441 -0.863171185425945 0.342627053981254 -0.332627704725983
    2.2137539,1.11226993252773 1.06484916245061 0.555266033439982 0.877691038550889 1.89254797819741 1.43890404648442 0.342627053981254 0.376490698755783
    2.2772673,-0.468768642850639 -1.43754788774533 -1.05652863719378 0.576050411655607 -0.522940888712441 0.0120483832567209 0.342627053981254 -0.687186906466865
    2.2975726,-0.618884859896728 -1.1366360750781 -0.519263746982526 -1.02470580167082 -0.522940888712441 -0.863171185425945 3.11219574032972 1.97200710658975
    2.3272777,-0.651431999123483 0.55329161145762 -0.250631301876899 1.11210019001038 -0.522940888712441 -0.179808625688859 -1.04215728919298 -0.864466507337306
    2.5217206,0.115499102435224 -0.512233676577595 0.286633588334355 1.13650173283446 -0.522940888712441 -0.179808625688859 0.342627053981254 -0.155348103855541
    2.5533438,0.266341329949937 -0.551137885443386 -0.384947524429713 0.354857790686005 -0.522940888712441 -0.863171185425945 0.342627053981254 -0.332627704725983
    2.5687881,1.16902610257751 0.855491905752846 2.03274448152093 1.22628985326088 1.89254797819741 2.02833774827712 3.11219574032972 2.68112551007152
    2.6567569,-0.218972367124187 0.851192298581141 0.555266033439982 -1.02470580167082 -0.522940888712441 -0.863171185425945 0.342627053981254 0.908329501367106
    2.677591,0.263121415733908 1.4142681068416 0.018001143228728 1.35980653053822 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    2.7180005,-0.0704736333296423 1.52000996595417 0.286633588334355 1.39364261119802 -0.522940888712441 -0.863171185425945 0.342627053981254 -0.332627704725983
    2.7942279,-0.751957286017338 0.316843561689933 -1.99674219506348 0.911736065044475 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    2.8063861,-0.685277652430997 1.28214038482516 0.823898478545609 0.232904453212813 -0.522940888712441 -0.863171185425945 0.342627053981254 -0.155348103855541
    2.8124102,-0.244991501432929 0.51882005949686 -0.384947524429713 0.823246560137838 -0.522940888712441 -0.863171185425945 0.342627053981254 0.553770299626224
    2.8419982,-0.75731027755674 2.09041984898851 1.22684714620405 1.53428167116843 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    2.8535925,1.20962937075363 -0.242882661178889 1.09253092365124 -1.02470580167082 -0.522940888712441 1.24263233939889 3.11219574032972 2.50384590920108
    2.9204698,0.570886990493502 0.58243883987948 0.555266033439982 1.16006887775962 -0.522940888712441 1.07357183940747 0.342627053981254 1.61744790484887
    2.9626924,0.719758684343624 0.984970304132004 1.09253092365124 1.52137230773457 -0.522940888712441 -0.179808625688859 0.342627053981254 -0.509907305596424
    2.9626924,-1.52406140158064 1.81975700990333 0.689582255992796 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    2.9729753,-0.132431544081234 2.68769877553723 1.09253092365124 1.53428167116843 -0.522940888712441 -0.442797990776478 0.342627053981254 -0.687186906466865
    3.0130809,0.436161292804989 -0.0834447307428255 -0.519263746982526 -1.02470580167082 1.89254797819741 1.07357183940747 0.342627053981254 1.26288870310799
    3.0373539,-0.161195191984091 -0.671900359186746 1.7641120364153 1.13650173283446 -0.522940888712441 -0.863171185425945 0.342627053981254 0.0219314970149
    3.2752562,1.39927182372944 0.513852869452676 0.689582255992796 -1.02470580167082 1.89254797819741 1.49394503405693 0.342627053981254 -0.155348103855541
    3.3375474,1.51967002306341 -0.852203755696565 0.555266033439982 -0.104527297798983 1.89254797819741 1.85927724828569 0.342627053981254 0.908329501367106
    3.3928291,0.560725834706224 1.87867703391426 1.09253092365124 1.39364261119802 -0.522940888712441 0.486423065822545 0.342627053981254 1.26288870310799
    3.4355988,1.00765532502814 1.69426310090641 1.89842825896812 1.53428167116843 -0.522940888712441 -0.863171185425945 0.342627053981254 -0.509907305596424
    3.4578927,1.10152996153577 -0.10927271844907 0.689582255992796 -1.02470580167082 1.89254797819741 1.97630171771485 0.342627053981254 1.61744790484887
    3.5160131,0.100001934217311 -1.30380956369388 0.286633588334355 0.316555063757567 -0.522940888712441 0.28786643052924 0.342627053981254 0.553770299626224
    3.5307626,0.987291634724086 -0.36279314978779 -0.922212414640967 0.232904453212813 -0.522940888712441 1.79270085261407 0.342627053981254 1.26288870310799
    3.5652984,1.07158528137575 0.606453149641961 1.7641120364153 -0.432854616994416 1.89254797819741 0.528504607720369 0.342627053981254 0.199211097885341
    3.5876769,0.180156323255198 0.188987436375017 -0.519263746982526 1.09956763075594 -0.522940888712441 0.708239632330506 0.342627053981254 0.199211097885341
    3.6309855,1.65687973755377 -0.256675483533719 0.018001143228728 -1.02470580167082 1.89254797819741 1.79270085261407 0.342627053981254 1.26288870310799
    3.6800909,0.5720085322365 0.239854450210939 -0.787896192088153 1.0605418233138 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    3.7123518,0.323806133438225 -0.606717660886078 -0.250631301876899 -1.02470580167082 1.89254797819741 0.342907418101747 0.342627053981254 0.199211097885341
    3.9843437,1.23668206715898 2.54220539083611 0.152317365781542 -1.02470580167082 1.89254797819741 1.89037692416194 0.342627053981254 1.26288870310799
    3.993603,0.180156323255198 0.154448192444669 1.62979581386249 0.576050411655607 1.89254797819741 0.708239632330506 0.342627053981254 1.79472750571931
    4.029806,1.60906277046565 1.10378605019827 0.555266033439982 -1.02470580167082 -0.522940888712441 -0.863171185425945 -1.04215728919298 -0.864466507337306
    4.1295508,1.0036214996026 0.113496885050331 -0.384947524429713 0.860016436332751 1.89254797819741 -0.863171185425945 0.342627053981254 -0.332627704725983
    4.3851468,1.25591974271076 0.577607033774471 0.555266033439982 -1.02470580167082 1.89254797819741 1.07357183940747 0.342627053981254 1.26288870310799
    4.6844434,2.09650591351268 0.625488598331018 -2.66832330782754 -1.02470580167082 1.89254797819741 1.67954222367555 0.342627053981254 0.553770299626224
    5.477509,1.30028987435881 0.338383613253713 0.555266033439982 1.00481276295349 1.89254797819741 1.24263233939889 0.342627053981254 1.97200710658975


3.2、代码三：

package mllib

import java.text.SimpleDateFormat
import java.util.Date

import org.apache.log4j.{Level, Logger}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LinearRegressionWithSGD, LabeledPoint}
import org.apache.spark.{SparkContext, SparkConf}

/**
  * 计算回归曲线的MSE
  * 对多组数据进行model的training,然后再利用model来predict具体的值
  * 过程中有输出model的权重
  * 公式：f(x)=a1X1+a2X2+a3X3+……
  * Created by 汪本成 on 2016/8/7.
  */
object LinearRegression2 {

  //屏蔽不必要的日志显示在终端上
  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getLogger("org.apache.eclipse.jetty.server").setLevel(Level.OFF)

  //程序入口
  val conf = new SparkConf()
    .setMaster("local[1]")
    .setAppName(this.getClass().getSimpleName().filter(!_.equals('$')))
  println(this.getClass().getSimpleName().filter(!_.equals('$')))

  val sc = new SparkContext(conf)

  def main(args: Array[String]) {
    //获取数据集路径
    val data = sc.textFile("G:\\MLlibData\\lpsa.data", 1)
    //处理数据集
    val parsedData = data.map{ line =>
      val parts = line.split(",")
      LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
    }



    //建立模型
    //建立model的数据和predict的数据没有分开
    val numIterations = 100
    val model = LinearRegressionWithSGD.train(parsedData, numIterations, 0.1)
    //for (i <- parsedData) println(i.label + ":" + i.features)

    //获取真实值与预测值
    val valuesAndPreds = parsedData.map { point =>
      //对系数进行预测
      val  prediction = model.predict(point.features)
      //按格式进行储存
      (point.label, prediction)
    }

    //打印权重
    var weights = model.weights
    println("model.weights" + weights)

    //save as file
    val isString = new SimpleDateFormat("yyyyMMddHHmmssSSS").format(new Date())
    val path = "G:\\MLlibData\\saveFile\\" + isString + "\\results"
    valuesAndPreds.saveAsTextFile(path)
    val MSE = valuesAndPreds.map {case(v, p) => math.pow((v - p), 2)}
      .reduce(_ + _ ) / valuesAndPreds.count
    println("训练的数据集的均方误差是： " + MSE)
    sc.stop()

  }
}


注意：MLlib中的线性回归比较适合做一元线性回归而非多元线性回归，当回归系数比较多时，算法产生的过拟合现象较为严重。
4、逻辑回归
4.1、数据

这里包括了我写的意愿逻辑回归和多元逻辑回归，数据用的是Spark工程下的sample_libsvm_data.txt文件和我自己弄的logisticRegression1.data,内容如下：

[plain] view plain copy

    1|2
    1|3
    1|4
    1|5
    1|6
    0|7
    0|8
    0|9
    0|10
    0|11


4.2、代码四

package mllib

import org.apache.log4j.{Level, Logger}
import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithSGD}
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkContext, SparkConf}

/**
  * 逻辑回归
  * Created by 汪本成 on 2016/8/7.
  */
object LogisticRegression {

  //屏蔽不必要的日志显示在终端上
  Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
  Logger.getLogger("org.apache.eclipse.jetty.server").setLevel(Level.OFF)


  val conf = new SparkConf()
    .setMaster("local[4]")
    .setAppName(this.getClass().getSimpleName().filter(!_.equals('$')))

  val sc = new SparkContext(conf)

  var logisticRegression = new LogisticRegression

  //一元逻辑回归数据集
  val LR1_PATH = "file\\data\\mllib\\input\\regression\\logisticRegression1.data"
  //多元逻辑回归数据集
  val LR2_PATH = "file\\data\\mllib\\input\\regression\\sample_libsvm_data.txt"

  val data = sc.textFile(LR1_PATH)
  val svmData = MLUtils.loadLibSVMFile(sc, LR2_PATH)

  //分割数据集
  val splits = svmData.randomSplit(Array(0.6, 0.4), seed = 11L)
  val parsedData_SVM = splits(0)
  val parsedTest_SVM = splits(1)


  //转化数据格式
  val parsedData = data.map { line =>
    val parts = line.split('|')
    LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
  }.cache()

  //建立模型
  val model = LogisticRegressionWithSGD.train(parsedData, 50)
  val svmModel = LogisticRegressionWithSGD.train(parsedData_SVM, 50)


  //创建测试值
  val target = Vectors.dense(-1)

  //根据模型计算测试值结果
  val predict = model.predict(target)

  //计算多元逻辑回归的测试值，并存储测试和预测值
  val predict_svm = logisticRegression.predictAndLabels(parsedTest_SVM, svmModel)

  //创建验证类
  val metrics = new MulticlassMetrics(predict_svm)

  //计算验证值
  val precision = metrics.precision

  def main(args: Array[String]) {
    println("一元逻辑回归:")
    parsedData.foreach(println)
    //打印权重
    println("权重: " + model.weights)
    println(predict)
    println(model.predict(Vectors.dense(10)))

    println("*************************************************************")

    println("多元逻辑回归:")
    println("svmData记录数：" + svmData.count())
    println("parsedData_SVM：" + parsedData_SVM.count())
    println("parsedTest_SVM：" + parsedTest_SVM.count())
    println("Precision = " + precision) //打印验证值
    predict_svm.take(10).foreach(println)
    println("权重: " + svmModel.weights)
    println("weights 个数是: " + svmModel.weights.size)
    //打印weight不为0个数
    println("weights不为0的个数是: " + model.weights.toArray.filter(_ != 0).size)
    sc.stop()
  }

}

class LogisticRegression {

  /**
    *
    * @param data  svmData
    * @param model LogisticRegressionModel
    * @return
    */
  def predictAndLabels(
  data: RDD[LabeledPoint],
  model: LogisticRegressionModel):RDD[(Double, Double)]= {
    val parsedData = data.map {
      point =>
        val prediction = model.predict(point.features)
        (point.label, prediction)
    }
    parsedData
  }
}

===============================================



1、MLlib实例
1.1 聚类实例
1.1.1 算法说明

聚类（Cluster analysis）有时也被翻译为簇类，其核心任务是：将一组目标object划分为若干个簇，每个簇之间的object尽可能相似，簇与簇之间的object尽可能相异。聚类算法是机器学习（或者说是数据挖掘更合适）中重要的一部分，除了最为简单的K-Means聚类算法外，比较常见的还有层次法（CURE、CHAMELEON等）、网格算法（STING、WaveCluster等），等等。

较权威的聚类问题定义：所谓聚类问题，就是给定一个元素集合D，其中每个元素具有n个可观察属性，使用某种算法将D划分成k个子集，要求每个子集内部的元素之间相异度尽可能低，而不同子集的元素相异度尽可能高。其中每个子集叫做一个簇。

K-means聚类属于无监督学习，以往的回归、朴素贝叶斯、SVM等都是有类别标签y的，也就是说样例中已经给出了样例的分类。而聚类的样本中却没有给定y，只有特征x，比如假设宇宙中的星星可以表示成三维空间中的点集clip_image002。聚类的目的是找到每个样本x潜在的类别y，并将同类别y的样本x放在一起。比如上面的星星，聚类后结果是一个个星团，星团里面的点相互距离比较近，星团间的星星距离就比较远了。

与分类不同，分类是示例式学习，要求分类前明确各个类别，并断言每个元素映射到一个类别。而聚类是观察式学习，在聚类前可以不知道类别甚至不给定类别数量，是无监督学习的一种。目前聚类广泛应用于统计学、生物学、数据库技术和市场营销等领域，相应的算法也非常多。
1.1.2 实例介绍

在该实例中将介绍K-Means算法，K-Means属于基于平方误差的迭代重分配聚类算法，其核心思想十分简单：

l随机选择K个中心点；

l计算所有点到这K个中心点的距离，选择距离最近的中心点为其所在的簇；

l简单地采用算术平均数（mean）来重新计算K个簇的中心；

l重复步骤2和3，直至簇类不再发生变化或者达到最大迭代值；

l输出结果。

K-Means算法的结果好坏依赖于对初始聚类中心的选择，容易陷入局部最优解，对K值的选择没有准则可依循，对异常数据较为敏感，只能处理数值属性的数据，聚类结构可能不平衡。

本实例中进行如下步骤：

1.装载数据，数据以文本文件方式进行存放；

2.将数据集聚类，设置2个类和20次迭代，进行模型训练形成数据模型；

3.打印数据模型的中心点；

4.使用误差平方之和来评估数据模型；

5.使用模型测试单点数据；

6.交叉评估1，返回结果；交叉评估2，返回数据集和结果。
1.1.3测试数据说明

该实例使用的数据为kmeans_data.txt，可以在本系列附带资源/data/class8/目录中找到。在该文件中提供了6个点的空间位置坐标，使用K-means聚类对这些点进行分类。

使用的kmeans_data.txt的数据如下所示：

0.0 0.0 0.0

0.1 0.1 0.1

0.2 0.2 0.2

9.0 9.0 9.0

9.1 9.1 9.1

9.2 9.2 9.2
1.1.4程序代码

import org.apache.log4j.{Level, Logger}

import org.apache.spark.{SparkConf, SparkContext}

import org.apache.spark.mllib.clustering.KMeans

import org.apache.spark.mllib.linalg.Vectors



object Kmeans {

  def main(args: Array[String]) {

    // 屏蔽不必要的日志显示在终端上

    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)

    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)



    // 设置运行环境

    val conf = new SparkConf().setAppName("Kmeans").setMaster("local[4]")

    val sc = new SparkContext(conf)



    // 装载数据集

    val data = sc.textFile("/home/hadoop/upload/class8/kmeans_data.txt", 1)

    val parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble)))



    // 将数据集聚类，2个类，20次迭代，进行模型训练形成数据模型

    val numClusters = 2

    val numIterations = 20

    val model = KMeans.train(parsedData, numClusters, numIterations)



    // 打印数据模型的中心点

    println("Cluster centers:")

    for (c <- model.clusterCenters) {

      println("  " + c.toString)

    }



    // 使用误差平方之和来评估数据模型

    val cost = model.computeCost(parsedData)

    println("Within Set Sum of Squared Errors = " + cost)



    // 使用模型测试单点数据

println("Vectors 0.2 0.2 0.2 is belongs to clusters:" + model.predict(Vectors.dense("0.2 0.2 0.2".split(' ').map(_.toDouble))))

println("Vectors 0.25 0.25 0.25 is belongs to clusters:" + model.predict(Vectors.dense("0.25 0.25 0.25".split(' ').map(_.toDouble))))

println("Vectors 8 8 8 is belongs to clusters:" + model.predict(Vectors.dense("8 8 8".split(' ').map(_.toDouble))))



    // 交叉评估1，只返回结果

    val testdata = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble)))

    val result1 = model.predict(testdata)

   result1.saveAsTextFile("/home/hadoop/upload/class8/result_kmeans1")



    // 交叉评估2，返回数据集和结果

    val result2 = data.map {

      line =>

        val linevectore = Vectors.dense(line.split(' ').map(_.toDouble))

        val prediction = model.predict(linevectore)

        line + " " + prediction

    }.saveAsTextFile("/home/hadoop/upload/class8/result_kmeans2")



    sc.stop()

  }

}

clip_image004
1.1.5 IDEA执行情况

第一步   使用如下命令启动Spark集群

$cd /app/hadoop/spark-1.1.0

$sbin/start-all.sh

第二步   在IDEA中设置运行环境

在IDEA运行配置中设置Kmeans运行配置，由于读入的数据已经在程序中指定，故在该设置界面中不需要设置输入参数

clip_image006

第三步   执行并观察输出

在运行日志窗口中可以看到，通过计算计算出模型并找出两个簇中心点：(9.1，9.1，9.1)和(0.1，0.1，0.1)，使用模型对测试点进行分类求出分属于族簇。

clip_image008

第四步   查看输出结果文件

在/home/hadoop/upload/class8目录中有两个输出目录：

clip_image010

查看结果1，在该目录中只输出了结果，分别列出了6个点所属不同的族簇

clip_image012

查看结果2，在该目录中输出了数据集和结果

clip_image014
1.2 回归算法实例
1.2.1 算法说明

线性回归是利用称为线性回归方程的函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析方法，只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归，在实际情况中大多数都是多元回归。

线性回归（Linear Regression）问题属于监督学习（Supervised Learning）范畴，又称分类（Classification）或归纳学习（Inductive Learning）。这类分析中训练数据集中给出的数据类型是确定的。机器学习的目标是，对于给定的一个训练数据集，通过不断的分析和学习产生一个联系属性集合和类标集合的分类函数（Classification Function）或预测函数）Prediction Function），这个函数称为分类模型（Classification Model——或预测模型（Prediction Model）。通过学习得到的模型可以是一个决策树、规格集、贝叶斯模型或一个超平面。通过这个模型可以对输入对象的特征向量预测或对对象的类标进行分类。

回归问题中通常使用最小二乘（Least Squares）法来迭代最优的特征中每个属性的比重，通过损失函数（Loss Function）或错误函数（Error Function)定义来设置收敛状态，即作为梯度下降算法的逼近参数因子。
1.2.2 实例介绍

该例子给出了如何导入训练集数据，将其解析为带标签点的RDD，然后使用了LinearRegressionWithSGD 算法来建立一个简单的线性模型来预测标签的值，最后计算了均方差来评估预测值与实际值的吻合度。

线性回归分析的整个过程可以简单描述为如下三个步骤：

（1）寻找合适的预测函数，即上文中的 h(x) ，用来预测输入数据的判断结果。这个过程是非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数，若是非线性的则无法用线性回归来得出高质量的结果。

（2）构造一个Loss函数（损失函数），该函数表示预测的输出（h）与训练数据标签之间的偏差，可以是二者之间的差（h-y）或者是其他的形式（如平方差开方）。综合考虑所有训练数据的“损失”，将Loss求和或者求平均，记为 J(θ) 函数，表示所有训练数据预测值与实际类别的偏差。

（3）显然， J(θ) 函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到 J(θ) 函数的最小值。找函数的最小值有不同的方法，Spark中采用的是梯度下降法（stochastic gradient descent，SGD)。
1.2.3程序代码

import org.apache.log4j.{Level, Logger}

import org.apache.spark.{SparkContext, SparkConf}

import org.apache.spark.mllib.regression.LinearRegressionWithSGD

import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.linalg.Vectors



object LinearRegression {

  def main(args:Array[String]): Unit ={

    // 屏蔽不必要的日志显示终端上

    Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)

    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)



    // 设置运行环境

    val conf = new SparkConf().setAppName("Kmeans").setMaster("local[4]")

    val sc = new SparkContext(conf)



    // Load and parse the data

    val data = sc.textFile("/home/hadoop/upload/class8/lpsa.data")

    val parsedData = data.map { line =>

      val parts = line.split(',')

      LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))

    }



    // Building the model

    val numIterations = 100

    val model = LinearRegressionWithSGD.train(parsedData, numIterations)



    // Evaluate model on training examples and compute training error

    val valuesAndPreds = parsedData.map { point =>

      val prediction = model.predict(point.features)

      (point.label, prediction)

    }



    val MSE = valuesAndPreds.map{ case(v, p) => math.pow((v - p), 2)}.reduce (_ + _) / valuesAndPreds.count

    println("training Mean Squared Error = " + MSE)



    sc.stop()

  }

}

clip_image016
1.2.4 执行情况

第一步   启动Spark集群

$cd /app/hadoop/spark-1.1.0

$sbin/start-all.sh

第二步   在IDEA中设置运行环境

在IDEA运行配置中设置LinearRegression运行配置，由于读入的数据已经在程序中指定，故在该设置界面中不需要设置输入参数

clip_image018

第三步   执行并观察输出

clip_image020
1.3 协同过滤实例
1.3.1 算法说明

协同过滤（Collaborative Filtering，简称CF，WIKI上的定义是：简单来说是利用某个兴趣相投、拥有共同经验之群体的喜好来推荐感兴趣的资讯给使用者，个人透过合作的机制给予资讯相当程度的回应（如评分）并记录下来以达到过滤的目的，进而帮助别人筛选资讯，回应不一定局限于特别感兴趣的，特别不感兴趣资讯的纪录也相当重要。

协同过滤常被应用于推荐系统。这些技术旨在补充用户—商品关联矩阵中所缺失的部分。

MLlib 当前支持基于模型的协同过滤，其中用户和商品通过一小组隐性因子进行表达，并且这些因子也用于预测缺失的元素。MLLib 使用交替最小二乘法（ALS） 来学习这些隐性因子。

用户对物品或者信息的偏好，根据应用本身的不同，可能包括用户对物品的评分、用户查看物品的记录、用户的购买记录等。其实这些用户的偏好信息可以分为两类：

l  显式的用户反馈：这类是用户在网站上自然浏览或者使用网站以外，显式地提供反馈信息，例如用户对物品的评分或者对物品的评论。

l  隐式的用户反馈：这类是用户在使用网站是产生的数据，隐式地反映了用户对物品的喜好，例如用户购买了某物品，用户查看了某物品的信息，等等。

显式的用户反馈能准确地反映用户对物品的真实喜好，但需要用户付出额外的代价；而隐式的用户行为，通过一些分析和处理，也能反映用户的喜好，只是数据不是很精确，有些行为的分析存在较大的噪音。但只要选择正确的行为特征，隐式的用户反馈也能得到很好的效果，只是行为特征的选择可能在不同的应用中有很大的不同，例如在电子商务的网站上，购买行为其实就是一个能很好表现用户喜好的隐式反馈。

推荐引擎根据不同的推荐机制可能用到数据源中的一部分，然后根据这些数据，分析出一定的规则或者直接对用户对其他物品的喜好进行预测计算。这样推荐引擎可以在用户进入时给他推荐他可能感兴趣的物品。

MLlib目前支持基于协同过滤的模型，在这个模型里，用户和产品被一组可以用来预测缺失项目的潜在因子来描述。特别是我们实现交替最小二乘（ALS）算法来学习这些潜在的因子，在 MLlib 中的实现有如下参数：

l  numBlocks是用于并行化计算的分块个数（设置为-1时 为自动配置）；

l  rank是模型中隐性因子的个数；

l  iterations是迭代的次数；

l  lambda是ALS 的正则化参数；

l  implicitPrefs决定了是用显性反馈ALS 的版本还是用隐性反馈数据集的版本；

l  alpha是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准。

clip_image022
1.3.2 实例介绍

在本实例中将使用协同过滤算法对GroupLens Research（http://grouplens.org/datasets/movielens/）提供的数据进行分析，该数据为一组从20世纪90年末到21世纪初由MovieLens用户提供的电影评分数据，这些数据中包括电影评分、电影元数据（风格类型和年代）以及关于用户的人口统计学数据（年龄、邮编、性别和职业等）。根据不同需求该组织提供了不同大小的样本数据，不同样本信息中包含三种数据：评分、用户信息和电影信息。

对这些数据分析进行如下步骤：

1. 装载如下两种数据：

a)装载样本评分数据，其中最后一列时间戳除10的余数作为key，Rating为值；

b)装载电影目录对照表（电影ID->电影标题）

2.将样本评分表以key值切分成3个部分，分别用于训练 (60%，并加入用户评分), 校验 (20%), and 测试 (20%)

3.训练不同参数下的模型，并再校验集中验证，获取最佳参数下的模型

4.用最佳模型预测测试集的评分，计算和实际评分之间的均方根误差

5.根据用户评分的数据，推荐前十部最感兴趣的电影（注意要剔除用户已经评分的电影）
1.3.3 测试数据说明

在MovieLens提供的电影评分数据分为三个表：评分、用户信息和电影信息，在该系列提供的附属数据提供大概6000位读者和100万个评分数据，具体位置为/data/class8/movielens/data目录下，对三个表数据说明可以参考该目录下README文档。

1.评分数据说明（ratings.data)

该评分数据总共四个字段，格式为UserID::MovieID::Rating::Timestamp，分为为用户编号：：电影编号：：评分：：评分时间戳，其中各个字段说明如下：

l用户编号范围1~6040

l电影编号1~3952

l电影评分为五星评分，范围0~5

l评分时间戳单位秒

l每个用户至少有20个电影评分

使用的ratings.dat的数据样本如下所示：

1::1193::5::978300760

1::661::3::978302109

1::914::3::978301968

1::3408::4::978300275

1::2355::5::978824291

1::1197::3::978302268

1::1287::5::978302039

1::2804::5::978300719

2.用户信息(users.dat)

用户信息五个字段，格式为UserID::Gender::Age::Occupation::Zip-code，分为为用户编号：：性别：：年龄：：职业::邮编，其中各个字段说明如下：

l用户编号范围1~6040

l性别，其中M为男性，F为女性

l不同的数字代表不同的年龄范围，如：25代表25~34岁范围

l职业信息，在测试数据中提供了21中职业分类

l地区邮编

使用的users.dat的数据样本如下所示：

1::F::1::10::48067

2::M::56::16::70072

3::M::25::15::55117

4::M::45::7::02460

5::M::25::20::55455

6::F::50::9::55117

7::M::35::1::06810

8::M::25::12::11413

3.电影信息(movies.dat)

电影数据分为三个字段，格式为MovieID::Title::Genres，分为为电影编号：：电影名：：电影类别，其中各个字段说明如下：

l电影编号1~3952

l由IMDB提供电影名称，其中包括电影上映年份

l电影分类，这里使用实际分类名非编号，如：Action、Crime等

使用的movies.dat的数据样本如下所示：

1::Toy Story (1995)::Animation|Children's|Comedy

2::Jumanji (1995)::Adventure|Children's|Fantasy

3::Grumpier Old Men (1995)::Comedy|Romance

4::Waiting to Exhale (1995)::Comedy|Drama

5::Father of the Bride Part II (1995)::Comedy

6::Heat (1995)::Action|Crime|Thriller

7::Sabrina (1995)::Comedy|Romance

8::Tom and Huck (1995)::Adventure|Children's
1.3.4 程序代码

import java.io.File

import scala.io.Source

import org.apache.log4j.{Level, Logger}

import org.apache.spark.SparkConf

import org.apache.spark.SparkContext

import org.apache.spark.SparkContext._

import org.apache.spark.rdd._

import org.apache.spark.mllib.recommendation.{ALS, Rating, MatrixFactorizationModel}

object MovieLensALS {

  def main(args: Array[String]) {

    // 屏蔽不必要的日志显示在终端上

    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)

    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)

    if (args.length != 2) {

      println("Usage: /path/to/spark/bin/spark-submit --driver-memory 2g --class week7.MovieLensALS " +

        "week7.jar movieLensHomeDir personalRatingsFile")

      sys.exit(1)

    }

    // 设置运行环境

    val conf = new SparkConf().setAppName("MovieLensALS").setMaster("local[4]")

    val sc = new SparkContext(conf)

    // 装载用户评分，该评分由评分器生成

    val myRatings = loadRatings(args(1))

    val myRatingsRDD = sc.parallelize(myRatings, 1)

    // 样本数据目录

    val movieLensHomeDir = args(0)



    // 装载样本评分数据，其中最后一列Timestamp取除10的余数作为key，Rating为值,即(Int,Rating)

    val ratings = sc.textFile(new File(movieLensHomeDir, "ratings.dat").toString).map { line =>

      val fields = line.split("::")

      (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))

    }



    // 装载电影目录对照表（电影ID->电影标题）

    val movies = sc.textFile(new File(movieLensHomeDir, "movies.dat").toString).map { line =>

      val fields = line.split("::")

      (fields(0).toInt, fields(1))

    }.collect().toMap

    val numRatings = ratings.count()

    val numUsers = ratings.map(_._2.user).distinct().count()

    val numMovies = ratings.map(_._2.product).distinct().count()

    println("Got " + numRatings + " ratings from " + numUsers + " users on " + numMovies + " movies.")

    // 将样本评分表以key值切分成3个部分，分别用于训练 (60%，并加入用户评分), 校验 (20%), and 测试 (20%)

    // 该数据在计算过程中要多次应用到，所以cache到内存

    val numPartitions = 4

    val training = ratings.filter(x => x._1 < 6)

      .values

      .union(myRatingsRDD) //注意ratings是(Int,Rating)，取value即可

      .repartition(numPartitions)

      .cache()

    val validation = ratings.filter(x => x._1 >= 6 && x._1 < 8)

      .values

      .repartition(numPartitions)

      .cache()

    val test = ratings.filter(x => x._1 >= 8).values.cache()

    val numTraining = training.count()

    val numValidation = validation.count()

    val numTest = test.count()

    println("Training: " + numTraining + ", validation: " + numValidation + ", test: " + numTest)

    // 训练不同参数下的模型，并在校验集中验证，获取最佳参数下的模型

    val ranks = List(8, 12)

    val lambdas = List(0.1, 10.0)

    val numIters = List(10, 20)

    var bestModel: Option[MatrixFactorizationModel] = None

    var bestValidationRmse = Double.MaxValue

    var bestRank = 0

    var bestLambda = -1.0

    var bestNumIter = -1

    for (rank <- ranks; lambda <- lambdas; numIter <- numIters) {

      val model = ALS.train(training, rank, numIter, lambda)

      val validationRmse = computeRmse(model, validation, numValidation)

      println("RMSE (validation) = " + validationRmse + " for the model trained with rank = "

        + rank + ", lambda = " + lambda + ", and numIter = " + numIter + ".")

      if (validationRmse < bestValidationRmse) {

        bestModel = Some(model)

        bestValidationRmse = validationRmse

        bestRank = rank

        bestLambda = lambda

        bestNumIter = numIter

      }

    }



    // 用最佳模型预测测试集的评分，并计算和实际评分之间的均方根误差

    val testRmse = computeRmse(bestModel.get, test, numTest)

    println("The best model was trained with rank = " + bestRank + " and lambda = " + bestLambda  + ", and numIter = " + bestNumIter + ", and its RMSE on the test set is " + testRmse + ".")

    // create a naive baseline and compare it with the best model

    val meanRating = training.union(validation).map(_.rating).mean

    val baselineRmse =

      math.sqrt(test.map(x => (meanRating - x.rating) * (meanRating - x.rating)).mean)

    val improvement = (baselineRmse - testRmse) / baselineRmse * 100

    println("The best model improves the baseline by " + "%1.2f".format(improvement) + "%.")

    // 推荐前十部最感兴趣的电影，注意要剔除用户已经评分的电影

    val myRatedMovieIds = myRatings.map(_.product).toSet

    val candidates = sc.parallelize(movies.keys.filter(!myRatedMovieIds.contains(_)).toSeq)

    val recommendations = bestModel.get

      .predict(candidates.map((0, _)))

      .collect()

      .sortBy(-_.rating)

      .take(10)

    var i = 1

    println("Movies recommended for you:")

    recommendations.foreach { r =>

      println("%2d".format(i) + ": " + movies(r.product))

      i += 1

    }

  sc.stop()

  }



  /** 校验集预测数据和实际数据之间的均方根误差 **/

  def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], n: Long): Double = {

    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))

    val predictionsAndRatings = predictions.map(x => ((x.user, x.product), x.rating))

      .join(data.map(x => ((x.user, x.product), x.rating)))

      .values

    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).reduce(_ + _) / n)

  }



  /** 装载用户评分文件 **/

  def loadRatings(path: String): Seq[Rating] = {

    val lines = Source.fromFile(path).getLines()

    val ratings = lines.map { line =>

      val fields = line.split("::")

      Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)

    }.filter(_.rating > 0.0)

    if (ratings.isEmpty) {

      sys.error("No ratings provided.")

    } else {

      ratings.toSeq

    }

  }

}

clip_image024
1.3.5 IDEA执行情况

第一步   使用如下命令启动Spark集群

$cd /app/hadoop/spark-1.1.0

$sbin/start-all.sh

第二步   进行用户评分，生成用户样本数据

由于该程序中最终推荐给用户十部电影，这需要用户提供对样本电影数据的评分，然后根据生成的最佳模型获取当前用户推荐电影。用户可以使用/home/hadoop/upload/class8/movielens/bin/rateMovies程序进行评分，最终生成personalRatings.txt文件：

personalRatings.txt内容如下：

0::1::1::1419058353

0::780::4::1419058353

0::590::2::1419058353

0::1210::3::1419058353

0::648::4::1419058353

0::344::5::1419058353

0::165::3::1419058353

0::153::4::1419058353

0::597::2::1419058353

0::1580::3::1419058353

0::231::5::1419058353


clip_image026

第三步   在IDEA中设置运行环境

在IDEA运行配置中设置MovieLensALS运行配置，需要设置输入数据所在文件夹和用户的评分文件路径：

l  输入数据所在目录：输入数据文件目录，在该目录中包含了评分信息、用户信息和电影信息，这里设置为/home/hadoop/upload/class8/movielens/data/

l  用户的评分文件路径：前一步骤中用户对十部电影评分结果文件路径，在这里设置为/home/hadoop/upload/class8/movielens/personalRatings.txt

第四步   执行并观察输出

l  输出Got 1000209 ratings from 6040 users on 3706 movies，表示本算法中计算数据包括大概100万评分数据、6000多用户和3706部电影；

l  输出Training: 602252, validation: 198919, test: 199049，表示对评分数据进行拆分为训练数据、校验数据和测试数据，大致占比为6:2:2；

l  在计算过程中选择8种不同模型对数据进行训练，然后从中选择最佳模型，其中最佳模型比基准模型提供22.30%

RMSE (validation) = 0.8680885498009973 for the model trained with rank = 8, lambda = 0.1, and numIter = 10.

RMSE (validation) = 0.868882967482595 for the model trained with rank = 8, lambda = 0.1, and numIter = 20.

RMSE (validation) = 3.7558695311242833 for the model trained with rank = 8, lambda = 10.0, and numIter = 10.

RMSE (validation) = 3.7558695311242833 for the model trained with rank = 8, lambda = 10.0, and numIter = 20.

RMSE (validation) = 0.8663942501841964 for the model trained with rank = 12, lambda = 0.1, and numIter = 10.

RMSE (validation) = 0.8674684744165418 for the model trained with rank = 12, lambda = 0.1, and numIter = 20.

RMSE (validation) = 3.7558695311242833 for the model trained with rank = 12, lambda = 10.0, and numIter = 10.

RMSE (validation) = 3.7558695311242833 for the model trained with rank = 12, lambda = 10.0, and numIter = 20.

The best model was trained with rank = 12 and lambda = 0.1, and numIter = 10, and its RMSE on the test set is 0.8652326018300565.

The best model improves the baseline by 22.30%.

l  利用前面获取的最佳模型，结合用户提供的样本数据，最终推荐给用户如下影片：

Movies recommended for you:

 1: Bewegte Mann, Der (1994)

 2: Chushingura (1962)

 3: Love Serenade (1996)

 4: For All Mankind (1989)

 5: Vie est belle, La (Life is Rosey) (1987)

 6: Bandits (1997)

 7: King of Masks, The (Bian Lian) (1996)

 8: I'm the One That I Want (2000)

 9: Big Trees, The (1952)

10: First Love, Last Rites (1997)

clip_image028


2、参考资料

（1）Spark官网 mlllib说明  http://spark.apache.org/docs/1.1.0/mllib-guide.html

（2）《机器学习常见算法分类汇总》 http://www.ctocio.com/hotnews/15919.html



===============================


http://www.cnblogs.com/ksWorld/p/6891664.html
http://www.voidcn.com/article/p-etrokkig-cm.html


SparkMLlib学习之线性回归

（一）回归的概念

　　1，回归与分类的区别

　　　分类模型处理表示类别的离散变量，而回归模型则处理可以取任意实数的目标变量。但是二者基本的原则类似，都是通过确定一个模型，将输入特征映射到预测的输出。回归模型和分类模型都是监督学习的一种形式。

　　2.回归分类

　　　线性回归模型：本质上和对应的线性分类模型一样，唯一的区别是线性回归使用的损失函数、相关连接函数和决策函数不同。MLlib提供了标准的最小二乘回归模型在MLlib中，标准的最小二乘回归不使用正则化。但是应用到错误预测值的损失函数会将错误做平方，从而放大损失。这也意味着最小平方回归对数据中的异常点和过拟合非常敏感。因此对于分类器，我们通常在实际中必须应用一定程度的正则化。正则化分为：应用L2正则化时通常称为岭回归（ridge regression），应用L1正则化是称为LASSO（Least Absolute Shrinkage and Selection Operator）。

　　　决策树模型：决策树同样可以通过改变不纯度的度量方法用于回归分析

（二）SparkMLlib线性回归的应用

　　1，数据集的选择

　　　　http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset。

　　2.数据集的描述

　　　　此数据是根据一系列的特征预测每小时自行车租车次数，特征类型如下：

　　3，数据处理及构建模型

　　　　数据集中共有17 379个小时的记录。接下来的实验，我们会忽略记录中的 instant和 dteday 。忽略两个记录次数的变量 casual 和 registered ，只保留 cnt （ casual 和registered 的和）。最后就剩下12个变量，其中前8个是类型变量，后4个是归一化后的实数变量。对其中8个类型变量，我们使用之前提到的二元编码，剩下4个实数变量不做处理。另外一种二元变量化方法：http://blog.csdn.net/u010824591/article/details/50374904
复制代码

import org.apache.log4j.{Level, Logger}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by Damon on 17-5-22.
  */
object Regression {
  def main(args: Array[String]): Unit = {
    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)
    val conf =new SparkConf().setAppName("regression").setMaster("local[4]")
    val sc =new SparkContext(conf)
    //文件名
    val file_bike="hour_nohead.csv"
    //调用二元向量化方法
    val labeled_file=labeledFile(file_bike,sc)
    /*/*对目标值取对数*/
    val labeled_file1=labeled_file.map(point => LabeledPoint(math.log(point.label),point.features))
    */
    //构建线性回归模型，注该方法在：spark2.1.0已经抛弃了。。。。
    val model_liner=LinearRegressionWithSGD.train(labeled_file,10,0.1)
    //val categoricalFeaturesInfo = Map[Int,Int]()
    //val model_DT=DecisionTree.trainRegressor(labeled_file,categoricalFeaturesInfo,"variance",5,32)
    val predict_vs_train=labeled_file.map{
      point => (model_liner.predict(point.features),point.label)
//对目标取对数后的，预测方法
     /* point => (math.exp(model_liner.predict(point.features)),math.exp(point.label))*/
    }
    predict_vs_train.take(5).foreach(println(_))
   /*
(135.94648455498356,16.0)
(134.38058174607252,40.0)
(134.1840793861374,32.0)
(133.88699144084515,13.0)
(133.77899037657548,1.0)
   */
  def labeledFile(originFile:String,sc:SparkContext):RDD[LabeledPoint]={
    val file_load=sc.textFile(originFile)
    val file_split=file_load.map(_.split(","))
    /*构建映射类函数的方法:mapping*/
    def mapping(rdd:RDD[Array[String]],index:Int)=
      rdd.map(x => x(index)).distinct.zipWithIndex().collect.toMap
    /*存储每列映射方法mapping的maps集合*/
    var maps:Map[Int,Map[String,Long]] = Map()
    /* 生成maps*/
    for(i <- 2 until 10)
      maps += (i -> mapping(file_split,i))
    /*max_size表示每列的特征之和*/
    val max_size=maps.map(x =>x._2.size).sum
    val file_label=file_split.map{
      x =>
        var num:Int=0
        var size:Int=0
        /*构建长度为max_size+4的特征数组,初始值全为0*/
        val arrayOfDim=Array.ofDim[Double](max_size+4)
        for(j<-2 until 10) {
          num = maps(j)(x(j)).toInt
          if(j==2) size=0 else size += maps(j-1).size
          /*为特征赋值*/
          arrayOfDim(size+num)=1.0
        }
        /*添加后面4列归一化的特征*/
        for(j<-10 until 14)
        arrayOfDim(max_size+(j-10))=x(j).toDouble
        /*生成LabeledPoint类型*/
        LabeledPoint(x(14).toDouble+x(15).toDouble,Vectors.dense(arrayOfDim))
    }
    file_label
  }
}

复制代码

　　4，模型性能评价

　　　　(1) MSE是均方误差，是用作最小二乘回归的损失函数，表示所有样本预测值和实际值平方差的平均值。公式如下：



　　　　(2)RMSE是MSE的平方根　　　　

　　　　(3)平均绝对误差（MAE）：预测值与实际值的绝对值差的平均值

　　　　　　　　　　　　

　　　　(4) 均方根对数误差（RMSLE）：预测值和目标值进行对数变换后的RMSE.

代码如下：
复制代码

/*MSE是均方误差*/
    val mse=predict_vs_train.map(x => math.pow(x._1-x._2,2)).mean()
   /* 平均绝对误差（MAE）*/
    val mae=predict_vs_train.map(x => math.abs(x._1-x._2)).mean()
    /*均方根对数误差（RMSLE）*/
    val rmsle=math.sqrt(predict_vs_train.map(x => math.pow(math.log(x._1+1)-math.log(x._2+1),2)).mean())
    println(s"mse is $mse and mae is $mae and rmsle is $rmsle")
/*
mse is 29897.34020145107 and mae is 130.53255991178477 and rmsle is 1.4803867063174845
*/

复制代码

（三） 改进模型性能和参数调优

　　1，变换目标变量

　　　许多机器学习模型都会假设输入数据和目标变量的分布，比如线性模型的假设为正态分布，这里就将目标值取对数（还可以去sqrt处理）（将上文注释去掉）实现正态分布，结果如为：mse is 47024.572159822106 and mae is 149.28861881845546 and rmsle is 1.4525632598540426

　　将上述结果和原始数据训练的模型性能比较，可以看到我们提升了RMSLE的性能，但是却没有提升MSE和MAE的性能。

　　2.交叉验证

　　　1，创建训练集和测试集来评估参数

　　　2，调节参数来判断对线性模型的影响

迭代次数及步长的影响：
复制代码

//划分训练集和测试集
    val labeled_orign = labeled_file.randomSplit(Array(0.8, 0.2), 11L)
    val train_file = labeled_orign(0)
    val test_file = labeled_orign(1)
    /*调节迭代次数*/
    val Iter_Results = Seq(1, 5, 10, 20, 50, 100).map { param =>
      val model = LinearRegressionWithSGD.train(test_file, param, 0.01)
      val scoreAndLabels = test_file.map { point =>
        (model.predict(point.features), point.label)
      }
      val rmsle = math.sqrt(scoreAndLabels.map(x => math.pow(math.log(x._1) - math.log(x._2), 2)).mean)
      (s"$param lambda", rmsle)
    }
    /*迭代次数的结果输出*/
    Iter_Results.foreach { case (param, rmsl) => println(f"$param, rmsle = ${rmsl}")}
    /*调节步长数的大小*/
    val Step_Results = Seq(0.01, 0.025, 0.05, 0.1, 1.0).map { param =>
        val model = LinearRegressionWithSGD.train(test_file, 20, param)
        val scoreAndLabels = test_file.map { point =>
          (model.predict(point.features), point.label)
        }
        val rmsle = math.sqrt(scoreAndLabels.map(x => math.pow(math.log(x._1) - math.log(x._2), 2)).mean)
        (s"$param lambda", rmsle)
      }
    /*步长的结果输出*/
    Step_Results.foreach { case (param, rmsl) => println(f"$param, rmsle = ${rmsl}")}
/*results
1 lambda, rmsle = 2.9033629718241167
5 lambda, rmsle = 2.0102924520366092
10 lambda, rmsle = 1.7548482896314488
20 lambda, rmsle = 1.5785106813100764
50 lambda, rmsle = 1.461748782192306
100 lambda, rmsle = 1.4462810196387068
步长
0.01 lambda, rmsle = 1.5785106813100764
0.025 lambda, rmsle = 1.4478358250917658
0.05 lambda, rmsle = 1.5152549319928832
0.1 lambda, rmsle = 1.5687431700715837
1.0 lambda, rmsle = NaN
*/

复制代码

　　结果表明，随着迭代次数的增加，误差确实有所下降（即性能提高），并且下降速率和预期一样越来越小。可以看出为什么不使用默认步长来训练线性模型。其中默认步长为1.0，得到的RMSLE结果为 nan 。这说明SGD模型收敛到了最差的局部最优解。这种情况在步长较大的时候容易出现，原因是算法收敛太快而不能得到最优解。另外，小步长与相对较小的迭代次数（比如上面的10次）对应的训练模型性能一般较差。而较小的步长与较大的迭代次数下通常可以收敛得到较好的解。通常来讲，步长和迭代次数的设定需要权衡。较小的步长意味着收敛速度慢，需要较大的迭代次数。但是较大的迭代次数更加耗时，特别是在大数据集上。

　　还可以调节L1正则化和L2正则化参数。 MLlib目前支持两种正则化方法L1和L2。 L2正则化假设模型参数服从高斯分布，L2正则化函数比L1更光滑，所以更容易计算；L1假设模型参数服从拉普拉斯分布，L1正则化具备产生稀疏解的功能，从而具备Feature Selection的能力。（由于spark 2.1.0中的线性回归方法已经忽略了，就没去验证L1和L2对模型的影响）






======================================

https://zhuanlan.zhihu.com/p/24311565
Pipeline详解及Spark MLlib使用
刘玲源
刘玲源
9 个月前

本文中，我们介绍机器学习管道的概念。机器学习管道提供一系列基于数据框的高级的接口来帮助用户建立和调试实际的机器学习管道。
管道里的主要概念

MLlib提供标准的接口来使联合多个算法到单个的管道或者工作流，管道的概念源于scikit-learn项目。

1.数据框：机器学习接口使用来自Spark SQL的数据框形式数据作为数据集，它可以处理多种数据类型。比如，一个数据框可以有不同的列存储文本、特征向量、标签值和预测值。

2.转换器：转换器是将一个数据框变为另一个数据框的算法。比如，一个机器学习模型就是一个转换器，它将带有特征数据框转为预测值数据框。

3.估计器：估计器是拟合一个数据框来产生转换器的算法。比如，一个机器学习算法就是一个估计器，它训练一个数据框产生一个模型。

4.管道：一个管道串起多个转换器和估计器，明确一个机器学习工作流。

5.参数：管道中的所有转换器和估计器使用共同的接口来指定参数。

    数据框

机器学习算法可以应用于多种类型的数据，如向量、文本、图像和结构化数据。管道接口中采用来自Spark SQL的数据框来支持多种类型的数据。

可以查看Spark SQLdatatype reference来了解数据框支持的基础和结构化数据类型。除了Spark SQL指南中提到的数据类型外，数据框还可以使用机器学习向量类型。

可以显式地建立数据框或者隐式地从规则的RDD建立数据框，下面的代码将会给出示例。

数据框中的列需要命名。代码中的示例使用如“text”，“features“和”label“的名字。

    管道组件

转换器包含特征变化和学习模型。技术上来说，转化器通过方法transform()，在原始数据上增加一列或者多列来将一个数据框转为另一个数据框。如：

1.一个特征转换器输入一个数据框，读取一个文本列，将其映射为新的特征向量列。输出一个新的带有特征向量列的数据框。

2。一个学习模型转换器输入一个数据框，读取包括特征向量的列，预测每一个特征向量的标签。输出一个新的带有预测标签列的数据框。
估计器

估计器指用来拟合或者训练数据的学习算法或者任何算法。技术上说，估计器通过fit()方法，接受一个数据框产生一个模型。比如，逻辑回归就是一个估计器，通过fit()来产生一个逻辑回归模型。
管道组件的特性

转换器的transform()方法和估计器的fit()方法都是无状态性的。将来，有状态性的算法可能通过其他概念得到支持。

每个转换器或估计器实例有唯一的编号，这个特征在制定参数的时候非常有用。

    管道

在机器学习中，运行一系列算法来处理和学习数据是非常常见的。如一个文档数据的处理工作流可能包括下列步骤：

1.将文档氛围单个词语。

2.将每个文档中的词语转为数字化的特征向量。

3.使用特征向量和标签学习一个预测模型。

MLlib将上述的工作流描述为管道，它包含一系列需要被执行的有顺序的管道阶段（转换器和估计器）。本节中我们将会使用上述文档处理工作流作为例子。
工作原理

管道由一系列有顺序的阶段指定，每个状态时转换器或估计器。每个状态的运行是有顺序的，输入的数据框通过每个阶段进行改变。在转换器阶段，transform()方法被调用于数据框上。对于估计器阶段，fit()方法被调用来产生一个转换器，然后该转换器的transform()方法被调用在数据框上。

下面的图说明简单的文档处理工作流的运行。

上面的图示中，第一行代表管道处理的三个阶段。第一二个蓝色的阶段是转换器，第三个红色框中的逻辑回归是估计器。底下一行代表管道中的数据流，圆筒指数据框。管道的fit()方法被调用于原始的数据框中，里面包含原始的文档和标签。分词器的transform()方法将原始文档分为词语，添加新的词语列到数据框中。哈希处理的transform()方法将词语列转换为特征向量，添加新的向量列到数据框中。然后，因为逻辑回归是估计器，管道先调用逻辑回归的fit()方法来产生逻辑回归模型。如果管道还有其它更多阶段，在将数据框传入下一个阶段之前，管道会先调用逻辑回归模型的transform()方法。

整个管道是一个估计器。所以当管道的fit()方法运行后，会产生一个管道模型，管道模型是转换器。管道模型会在测试时被调用，下面的图示说明用法。

上面的图示中，管道模型和原始管道有同样数目的阶段，然而原始管道中的估计器此时变为了转换器。当管道模型的transform()方法被调用于测试数据集时，数据依次经过管道的各个阶段。每个阶段的transform()方法更新数据集，并将之传到下个阶段。

管道和管道模型有助于确认训练数据和测试数据经过同样的特征处理流程。

    详细信息

DAG管道：管道的状态是有序的队列。这儿给的例子都是线性的管道，也就是说管道的每个阶段使用上一个阶段产生的数据。我们也可以产生非线性的管道，数据流向为无向非环图(DAG)。这种图通常需要明确地指定每个阶段的输入和输出列名（通常以指定参数的形式）。如果管道是DAG形式，则每个阶段必须以拓扑序的形式指定。

运行时间检查：因为管道可以运行在多种数据类型上，所以不能使用编译时间检查。管道和管道模型在实际运行管道之前就会进行运行时间检查。这种检查通过数据框摘要，它描述了数据框中各列的类型。
唯一的管道阶段：管道的每个阶段需要是唯一的实体。如同样的实体“哈希变换”不可以进入管道两次，因为管道的每个阶段必须有唯一的ID。当然“哈希变换1”和“哈希变换2”（都是哈希变换类型）可以进入同个管道两次，因为他们有不同的ID。参数

MLlib估计器和转换器使用统一的接口来指定参数。

Param是有完备文档的已命名参数。ParamMap是一些列“参数－值”对。

有两种主要的方法来向算法传递参数：

1.给实体设置参数。比如，lr是一个逻辑回归实体，通过lr.setMaxIter(10)来使得lr在拟合的时候最多迭代10次。这个接口与spark.mllib包相似。

2.传递ParamMap到fit()或者transform()。所有在ParamMap里的参数都将通过设置被重写。

参数属于指定估计器和转换器实体过程。因此，如果我们有两个逻辑回归实体lr1和lr2，我们可以建立一个ParamMap来指定两个实体的最大迭代次数参数：ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20)。这在一个管道里有两个算法都有最大迭代次数参数时非常有用。

存储和读取管道

我们经常需要将管道存储到磁盘以供下次使用。在Spark1.6中，模型导入导出功能新添了管道接口，支持大多数转换器。请到算法接口文档查看是否支持存储和读入。

    代码示例

下面给出上述讨论功能的代码示例：

估计器、转换器和Param示例：

Scala:

import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.Row

// Prepare training data from a list of (label, features) tuples.
val training = spark.createDataFrame(Seq(
  (1.0, Vectors.dense(0.0, 1.1, 0.1)),
  (0.0, Vectors.dense(2.0, 1.0, -1.0)),
  (0.0, Vectors.dense(2.0, 1.3, 1.0)),
  (1.0, Vectors.dense(0.0, 1.2, -0.5))
)).toDF("label", "features")

// Create a LogisticRegression instance. This instance is an Estimator.
val lr = new LogisticRegression()
// Print out the parameters, documentation, and any default values.
println("LogisticRegression parameters:\n" + lr.explainParams() + "\n")

// We may set parameters using setter methods.
lr.setMaxIter(10)
  .setRegParam(0.01)

// Learn a LogisticRegression model. This uses the parameters stored in lr.
val model1 = lr.fit(training)
// Since model1 is a Model (i.e., a Transformer produced by an Estimator),
// we can view the parameters it used during fit().
// This prints the parameter (name: value) pairs, where names are unique IDs for this
// LogisticRegression instance.
println("Model 1 was fit using parameters: " + model1.parent.extractParamMap)

// We may alternatively specify parameters using a ParamMap,
// which supports several methods for specifying parameters.
val paramMap = ParamMap(lr.maxIter -> 20)
  .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.
  .put(lr.regParam -> 0.1, lr.threshold -> 0.55)  // Specify multiple Params.

// One can also combine ParamMaps.
val paramMap2 = ParamMap(lr.probabilityCol -> "myProbability")  // Change output column name.
val paramMapCombined = paramMap ++ paramMap2

// Now learn a new model using the paramMapCombined parameters.
// paramMapCombined overrides all parameters set earlier via lr.set* methods.
val model2 = lr.fit(training, paramMapCombined)
println("Model 2 was fit using parameters: " + model2.parent.extractParamMap)

// Prepare test data.
val test = spark.createDataFrame(Seq(
  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),
  (0.0, Vectors.dense(3.0, 2.0, -0.1)),
  (1.0, Vectors.dense(0.0, 2.2, -1.5))
)).toDF("label", "features")

// Make predictions on test data using the Transformer.transform() method.
// LogisticRegression.transform will only use the 'features' column.
// Note that model2.transform() outputs a 'myProbability' column instead of the usual
// 'probability' column since we renamed the lr.probabilityCol parameter previously.
model2.transform(test)
  .select("features", "label", "myProbability", "prediction")
  .collect()
  .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>
    println(s"($features, $label) -> prob=$prob, prediction=$prediction")
  }

Java:

import java.util.Arrays;
import java.util.List;

import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.classification.LogisticRegressionModel;
import org.apache.spark.ml.linalg.VectorUDT;
import org.apache.spark.ml.linalg.Vectors;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.Metadata;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

// Prepare training data.
List<Row> dataTraining = Arrays.asList(
    RowFactory.create(1.0, Vectors.dense(0.0, 1.1, 0.1)),
    RowFactory.create(0.0, Vectors.dense(2.0, 1.0, -1.0)),
    RowFactory.create(0.0, Vectors.dense(2.0, 1.3, 1.0)),
    RowFactory.create(1.0, Vectors.dense(0.0, 1.2, -0.5))
);
StructType schema = new StructType(new StructField[]{
    new StructField("label", DataTypes.DoubleType, false, Metadata.empty()),
    new StructField("features", new VectorUDT(), false, Metadata.empty())
});
Dataset<Row> training = spark.createDataFrame(dataTraining, schema);

// Create a LogisticRegression instance. This instance is an Estimator.
LogisticRegression lr = new LogisticRegression();
// Print out the parameters, documentation, and any default values.
System.out.println("LogisticRegression parameters:\n" + lr.explainParams() + "\n");

// We may set parameters using setter methods.
lr.setMaxIter(10).setRegParam(0.01);

// Learn a LogisticRegression model. This uses the parameters stored in lr.
LogisticRegressionModel model1 = lr.fit(training);
// Since model1 is a Model (i.e., a Transformer produced by an Estimator),
// we can view the parameters it used during fit().
// This prints the parameter (name: value) pairs, where names are unique IDs for this
// LogisticRegression instance.
System.out.println("Model 1 was fit using parameters: " + model1.parent().extractParamMap());

// We may alternatively specify parameters using a ParamMap.
ParamMap paramMap = new ParamMap()
  .put(lr.maxIter().w(20))  // Specify 1 Param.
  .put(lr.maxIter(), 30)  // This overwrites the original maxIter.
  .put(lr.regParam().w(0.1), lr.threshold().w(0.55));  // Specify multiple Params.

// One can also combine ParamMaps.
ParamMap paramMap2 = new ParamMap()
  .put(lr.probabilityCol().w("myProbability"));  // Change output column name
ParamMap paramMapCombined = paramMap.$plus$plus(paramMap2);

// Now learn a new model using the paramMapCombined parameters.
// paramMapCombined overrides all parameters set earlier via lr.set* methods.
LogisticRegressionModel model2 = lr.fit(training, paramMapCombined);
System.out.println("Model 2 was fit using parameters: " + model2.parent().extractParamMap());

// Prepare test documents.
List<Row> dataTest = Arrays.asList(
    RowFactory.create(1.0, Vectors.dense(-1.0, 1.5, 1.3)),
    RowFactory.create(0.0, Vectors.dense(3.0, 2.0, -0.1)),
    RowFactory.create(1.0, Vectors.dense(0.0, 2.2, -1.5))
);
Dataset<Row> test = spark.createDataFrame(dataTest, schema);

// Make predictions on test documents using the Transformer.transform() method.
// LogisticRegression.transform will only use the 'features' column.
// Note that model2.transform() outputs a 'myProbability' column instead of the usual
// 'probability' column since we renamed the lr.probabilityCol parameter previously.
Dataset<Row> results = model2.transform(test);
Dataset<Row> rows = results.select("features", "label", "myProbability", "prediction");
for (Row r: rows.collectAsList()) {
  System.out.println("(" + r.get(0) + ", " + r.get(1) + ") -> prob=" + r.get(2)
    + ", prediction=" + r.get(3));
}

Python:

from pyspark.ml.linalg import Vectors
from pyspark.ml.classification import LogisticRegression

# Prepare training data from a list of (label, features) tuples.
training = spark.createDataFrame([
    (1.0, Vectors.dense([0.0, 1.1, 0.1])),
    (0.0, Vectors.dense([2.0, 1.0, -1.0])),
    (0.0, Vectors.dense([2.0, 1.3, 1.0])),
    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], ["label", "features"])

# Create a LogisticRegression instance. This instance is an Estimator.
lr = LogisticRegression(maxIter=10, regParam=0.01)
# Print out the parameters, documentation, and any default values.
print "LogisticRegression parameters:\n" + lr.explainParams() + "\n"

# Learn a LogisticRegression model. This uses the parameters stored in lr.
model1 = lr.fit(training)

# Since model1 is a Model (i.e., a transformer produced by an Estimator),
# we can view the parameters it used during fit().
# This prints the parameter (name: value) pairs, where names are unique IDs for this
# LogisticRegression instance.
print "Model 1 was fit using parameters: "
print model1.extractParamMap()

# We may alternatively specify parameters using a Python dictionary as a paramMap
paramMap = {lr.maxIter: 20}
paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.
paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.

# You can combine paramMaps, which are python dictionaries.
paramMap2 = {lr.probabilityCol: "myProbability"}  # Change output column name
paramMapCombined = paramMap.copy()
paramMapCombined.update(paramMap2)

# Now learn a new model using the paramMapCombined parameters.
# paramMapCombined overrides all parameters set earlier via lr.set* methods.
model2 = lr.fit(training, paramMapCombined)
print "Model 2 was fit using parameters: "
print model2.extractParamMap()

# Prepare test data
test = spark.createDataFrame([
    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),
    (0.0, Vectors.dense([3.0, 2.0, -0.1])),
    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], ["label", "features"])

# Make predictions on test data using the Transformer.transform() method.
# LogisticRegression.transform will only use the 'features' column.
# Note that model2.transform() outputs a "myProbability" column instead of the usual
# 'probability' column since we renamed the lr.probabilityCol parameter previously.
prediction = model2.transform(test)
selected = prediction.select("features", "label", "myProbability", "prediction")
for row in selected.collect():
    print row

管道示例：

Scala:

import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.Row

// Prepare training documents from a list of (id, text, label) tuples.
val training = spark.createDataFrame(Seq(
  (0L, "a b c d e spark", 1.0),
  (1L, "b d", 0.0),
  (2L, "spark f g h", 1.0),
  (3L, "hadoop mapreduce", 0.0)
)).toDF("id", "text", "label")

// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val hashingTF = new HashingTF()
  .setNumFeatures(1000)
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.01)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))

// Fit the pipeline to training documents.
val model = pipeline.fit(training)

// Now we can optionally save the fitted pipeline to disk
model.write.overwrite().save("/tmp/spark-logistic-regression-model")

// We can also save this unfit pipeline to disk
pipeline.write.overwrite().save("/tmp/unfit-lr-model")

// And load it back in during production
val sameModel = PipelineModel.load("/tmp/spark-logistic-regression-model")

// Prepare test documents, which are unlabeled (id, text) tuples.
val test = spark.createDataFrame(Seq(
  (4L, "spark i j k"),
  (5L, "l m n"),
  (6L, "mapreduce spark"),
  (7L, "apache hadoop")
)).toDF("id", "text")

// Make predictions on test documents.
model.transform(test)
  .select("id", "text", "probability", "prediction")
  .collect()
  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
    println(s"($id, $text) --> prob=$prob, prediction=$prediction")
  }

Java:

import java.util.Arrays;

import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.feature.HashingTF;
import org.apache.spark.ml.feature.Tokenizer;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

// Prepare training documents, which are labeled.
Dataset<Row> training = spark.createDataFrame(Arrays.asList(
  new JavaLabeledDocument(0L, "a b c d e spark", 1.0),
  new JavaLabeledDocument(1L, "b d", 0.0),
  new JavaLabeledDocument(2L, "spark f g h", 1.0),
  new JavaLabeledDocument(3L, "hadoop mapreduce", 0.0)
), JavaLabeledDocument.class);

// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
Tokenizer tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words");
HashingTF hashingTF = new HashingTF()
  .setNumFeatures(1000)
  .setInputCol(tokenizer.getOutputCol())
  .setOutputCol("features");
LogisticRegression lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.01);
Pipeline pipeline = new Pipeline()
  .setStages(new PipelineStage[] {tokenizer, hashingTF, lr});

// Fit the pipeline to training documents.
PipelineModel model = pipeline.fit(training);

// Prepare test documents, which are unlabeled.
Dataset<Row> test = spark.createDataFrame(Arrays.asList(
  new JavaDocument(4L, "spark i j k"),
  new JavaDocument(5L, "l m n"),
  new JavaDocument(6L, "mapreduce spark"),
  new JavaDocument(7L, "apache hadoop")
), JavaDocument.class);

// Make predictions on test documents.
Dataset<Row> predictions = model.transform(test);
for (Row r : predictions.select("id", "text", "probability", "prediction").collectAsList()) {
  System.out.println("(" + r.get(0) + ", " + r.get(1) + ") --> prob=" + r.get(2)
    + ", prediction=" + r.get(3));
}

Python:

from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer

# Prepare training documents from a list of (id, text, label) tuples.
training = spark.createDataFrame([
    (0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0)], ["id", "text", "label"])

# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.01)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

# Fit the pipeline to training documents.
model = pipeline.fit(training)

# Prepare test documents, which are unlabeled (id, text) tuples.
test = spark.createDataFrame([
    (4, "spark i j k"),
    (5, "l m n"),
    (6, "mapreduce spark"),
    (7, "apache hadoop")], ["id", "text"])

# Make predictions on test documents and print columns of interest.
prediction = model.transform(test)
selected = prediction.select("id", "text", "prediction")
for row in selected.collect():
    print(row)

　　　　

===============================



我要做的是发现异常用户，而我们之前没有已经打上异常不异常标签的样本，所以对原始数据进行清洗、特征工程、降维后用Mahout/R做的聚类，
打上了标签，本来打算继续用Mahout/R做分类，Mahout太慢，而用R实现KNN、RandomForest之后发现无法应用到真实项目上，所以用了MLlib。

下面是用R获取正负样本

#1.将kmeans标出的异常类标为1作为正样本，其它类标为0作为负样本
#1.1正常类标为0，作为负样本
#%in% 用法 a %in% table a值是否包含于table中，为真输出TURE，否则输出FALSE
levels(t_knn$cluster)[levels(t_knn$cluster) %in% c("1","2","11","4","5","6","7","8","9","10","12","13","14","15","16")] <- "0"

#1.2异常类标为1，作为正样本
levels(t_knn$cluster)[levels(t_knn$cluster) %in% c("3")] <- "1"
#1.3获取正负样本
negative_samples<-t_knn[which(t_knn$cluster=="0"),]
positive_samples<-t_knn[which(t_knn$cluster=="1"),]
#levels(negative_samples$cluster)

#################################################################################
#2.从正负样本中各随机抽取250个样本，组成500样本集v3
set.seed(1)
#sample(x, size, replace = FALSE, prob = NULL)默认不放回抽样
ns <- negative_samples[sample(1:nrow(negative_samples),250),]
ps <- positive_samples[sample(1:nrow(positive_samples),250),]
v3 <- rbind(ps,ns)
#levels(v3$cluster)
#去除第一列，第一列为用户id
vv3<-v3[,2:23]
#写入本地
#write.table(vv3, file ="v3.csv",sep = ',',row.names = F, col.names = F, quote = F)

将v3.csv存入HDFS

下面是Spark MLlib RandomForest代码实现

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.rdd.RDD

/** * Created by drguo on 2016/11/18. */
object RunRF {
  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("rf")
    val sc = new SparkContext(sparkConf)
    //读取数据
    val rawData = sc.textFile("hdfs://192.168.1.64:8020/test/mllib/v3.csv")
    val data = rawData.map{ line =>
      val values = line.split(",").map(_.toDouble)
      //init返回除了最后一个元素的所有元素,作为特征向量
      //Vectors.dense向量化，dense密集型
      val feature = Vectors.dense(values.init)
      val label = values.last
      LabeledPoint(label, feature)
    }
    //训练集、交叉验证集和测试集，各占80%，10%和10%
    //10%的交叉验证数据集的作用是确定在训练数据集上训练出来的模型的最好参数
    //测试数据集的作用是评估CV数据集的最好参数
    val Array(trainData, cvData, testData) = data.randomSplit(Array(0.8, 0.1, 0.1))
    trainData.cache()
    cvData.cache()
    testData.cache()

    //构建随机森林
    val model = RandomForest.trainClassifier(trainData, 2, Map[Int, Int](), 20, "auto", "gini", 4, 32)
    val metrics = getMetrics(model, cvData)
    println("-----------------------------------------confusionMatrix-----------------------------------------------------")
    //混淆矩阵和模型精确率
    println(metrics.confusionMatrix)
    println("---------------------------------------------precision-------------------------------------------------")
    println(metrics.precision)

    println("-----------------------------------------(precision,recall)---------------------------------------------------")
    //每个类别对应的精确率与召回率
    (0 until 2).map(target => (metrics.precision(target), metrics.recall(target))).foreach(println)
    //保存模型
    model.save(sc,"hdfs://192.168.1.64:8020/tmp/RFModel")

  }

  /** * @param model 随机森林模型 * @param data 用于交叉验证的数据集 * */
  def getMetrics(model: RandomForestModel, data: RDD[LabeledPoint]): MulticlassMetrics = {
    //将交叉验证数据集的每个样本的特征向量交给模型预测,并和原本正确的目标特征组成一个tuple
    val predictionsAndLables = data.map { d =>
      (model.predict(d.features), d.label)
    }
    //将结果交给MulticlassMetrics,其可以以不同的方式计算分配器预测的质量
    new MulticlassMetrics(predictionsAndLables)
  }
  /** * 在训练数据集上得到最好的参数组合 * @param trainData 训练数据集 * @param cvData 交叉验证数据集 * */
  def getBestParam(trainData: RDD[LabeledPoint], cvData: RDD[LabeledPoint]): Unit = {
    val evaluations = for (impurity <- Array("gini", "entropy");
                           depth <- Array(1, 20);
                           bins <- Array(10, 300)) yield {
      val model = RandomForest.trainClassifier(trainData, 2, Map[Int, Int](), 20, "auto", impurity, depth, bins)
      val metrics = getMetrics(model, cvData)
      ((impurity, depth, bins), metrics.precision)
    }
    evaluations.sortBy(_._2).reverse.foreach(println)
  }

  /** * 模拟对新数据进行预测1 */
  val rawData = sc.textFile("hdfs://192.168.1.64:8020/test/mllib/v3.csv")

  val pdata = rawData.map{ line =>
    val values = line.split(",").map(_.toDouble)
    //转化为向量并去掉标签(init去掉最后一个元素,即去掉标签)
    val feature = Vectors.dense(values.init)
    feature
  }
  //读取模型
  val rfModel = RandomForestModel.load(sc,"hdfs://192.168.1.64:8020/tmp/RFModel")
  //进行预测
  val preLabel = rfModel.predict(pdata)
  preLabel.take(10)
  /** * 模拟对新数据进行预测2 * */
  val dataAndPreLable = rawData.map{ line =>
    //转化为向量并去掉标签(init去掉最后一个元素,即去掉标签)
    val vecData = Vectors.dense(line.split(",").map(_.toDouble).init)
    val preLabel = rfModel.predict(vecData)
    line + "," + preLabel
  }//.saveAsTextFile("....")
dataAndPreLable.take(10)
}