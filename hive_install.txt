　由于Hive依赖于Hadoop，安装Hive之前必须确认Hadoop可用，关于Hadoop的安装可以参考集群分布式 Hadoop安装详细步骤，这里不再叙述。
1.下载Hive安装包
　　下载地址为：http://www.apache.org/dyn/closer.cgi/hive（或点击下面链接），选择一个稳定版本，假设下载的版本为：apache-hive-1.0.1-bin.tar.gz，解压：
tar -zxvf apache-hive-1.0.1-bin.tar.gz
　　在apache的下载列表中可以看到hive-1.0.1.src.tar.gz和hive-hive-1.0.1.bin.tar.gz两个包，名字中带bin的压缩包中只包括已经编译好的的Hive程序，不包括Hive的源代码。
2.配置环境变量
　　实际上即使不对操作系统的环境变量进行配置，Hive依然可以使用，但是如果每次都输入全路径效率很低，因此推荐对Linux操作系统的环境变量按以下方式进行配置。
修改全局配置文件/etc/profile或者用户目录下的私有文件~/.bashrc，在文件中加入以下信息：
export HIVE_HOME=/home/hadoop/apache-hive-1.0.1-bin
export PATH=$HIVE_HOME/bin:$HIVE_HOME/conf:$PATH
为了让配置立即生效，而不需要重新启动系统或者重新登录，运行以下命令：
source /etc/profile 或
source ~/.bashrc
3.创建Hive数据文件目录
在HDFS中建立用于存储Hive数据的文件目录(/tmp 目录可能已经存在)：
hadoop fs -mkdir /tmp
hadoop fs -mkdir /user/hive/warehouse
hadoop fs -chmod 777 /tmp
hadoop fs -chmod 777 /user/hive/warehouse
　　以上命令在HDFS中建立了/tmp及/usr/hive/warehouse目录，其中/tmp主要用于存放一些执行过程中的临时文件，/user/hive/warehouse用于存放hive进行管理的数据文件。
4.修改hive配置文件
　　这一步不是必须的，如果不配置，Hive将使用默认的配置文件，通过Hive配置文件可以对Hive进行定制及优化。最常见的是对“元数据存储层”的配置，默认情况下Hive使用Derby数据库作为“元数据存储层”。
在Hive中Derby默认使用“单用户”模式进行启动，这就意味着同一时间只能有一个用户使用Hive，这适用于开发程序时做本地测试。
　　Hive配置文件位于$Hive_Home/conf目录下面，名为hive-site.xml，这个文件默认情况下是不存在的，需要进行手动创建，在此目录下有个hive-default.xml.template的模板文件，首先需要通过它创建hive-site.xml文件。
cp hive-default.xml.template hive-site.xml
关于元数据库Dergy的默认配置如下：
<!--JDBC元数据仓库连接字符串-->
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:;databaseName=metastore_db;create=true</value>
    <description>JDBC connect string for a JDBC metastore</description>
  </property>
  <!--JDBC元数据仓库驱动类名-->
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.apache.derby.jdbc.EmbeddedDriver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>
  <!--元数据仓库用户名-->
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>APP</value>
    <description>Username to use against metastore database</description>
  </property>
   <!--元数据仓库密码-->
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>mine</value>
    <description>password to use against metastore database</description>
  </property>
　　从上面的配置中可以看到关于元数据库的配置，由于Hive中已经包含了这个内置的Derby数据库，因此不需要进行数据库的安装，同时在$Hive_Home/lib下还可以看到Derby的数据库驱动包（derby-xx.x.x.x.jar）。至此已经完成Hive各项工作的安装，可以通过以下命令测试Hive是否正常运行：
hive
hive> SET -v;
hive> quit;
出错处理：Exception in thread “main” Java.lang.RuntimeException: java. lang. IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI:
${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:444)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:672)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI:
处理：
在hive-site.xml中新建配置项，iotmp文件夹是新建的：
<property>
<name>system:java.io.tmpdir</name>
<value>/home/hadoop/hive-1.0.1/iotmp</value>
<description/>
</property>
下面是配置MySQL 元数据库，替换Derby
　　如前所述，Hive在缺省情况下是使用内置的Derty数据库存储元数据，这对程序开发时本地测试没有任何问题。但如果在生产环境中，由于需要支持多用户同时进行系统访问，这可能不能满足应用需求。通过配置，可以让Derty运行为“多用户”模式来满足多用户访问需求。进一步，在实际的生产环境中通常会选用存储功能更为强大的Mysql数据库作为“元数据存储层”。Mysql作为最流行的开源关系型数据库，使用面广、功能多样，必要时可以充当临时的标准数据查询与分析系统使用，因此得到大量的Hive用户的青睐。
如果使用Mysql作为“元数据存储层”，首先需要安装Mysql，可以使用如下命令：
#sudo apt-get install mysql-server
安装之后再数据库中建立Hive账号并设置权限：
mysql> create user 'hive'@'%' identified by 'hive';
mysql> grant all privileges on *.* to 'hive'@'%' with grant option;
mysql> flush privileges;
接下来需要对hive-sive.xml配置文件进行以下修改，以支持mysql:
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost:3306/metastore_db?createDatabaseIfNotExist=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
  <description>username to use against metastore database</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
  <description>password to use against metastore database</description>
</property>
　　另外，由于Hive没有默认包含Mysql的JDBC驱动，因此需要将mysql-connector-java-x.x.xx.jar文件拷贝到$Hive_Home/lib目录中，否则Hive无法与Mysql进行通信。至此，基于Mysql作为“元数据存储层”的Hive系统配置完成。(jar包下载需要注册oracle，需要的可以留言)
===================================
Hive是基于Hadoop构建的一套数据仓库分析系统，它提供了丰富的SQL查询方式来分析存储在Hadoop 分布式文件系统中的数据。其在Hadoop的架构体系中承担了一个SQL解析的过程，它提供了对外的入口来获取用户的指令然后对指令进行分析，解析出一个MapReduce程序组成可执行计划，并按照该计划生成对应的MapReduce任务提交给Hadoop集群处理，获取最终的结果。元数据——如表模式——存储在名为metastore的数据库中。
系统环境
192.168.186.128 hadoop-master
192.168.186.129 hadoop-slave
MySQL安装在master机器上，hive服务器也安装在master上
Hive下载
下载源码包，最新版本可自行去官网下载
[hadoop@hadoop-master ~]$ wget http://mirrors.cnnic.cn/apache/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz
[hadoop@hadoop-master ~]$ tar -zxf apache-hive-1.2.1-bin.tar.gz
[hadoop@hadoop-master ~]$ ls
apache-hive-1.2.1-bin  apache-hive-1.2.1-bin.tar.gz  dfs  hadoop-2.7.1  Hsource  tmp
配置环境变量
[root@hadoop-master hadoop]# vi /etc/profile
HIVE_HOME=/home/hadoop/apache-hive-1.2.1-bin
PATH=$PATH:$HIVE_HOME/bin
export HIVE_NAME PATH
[root@hadoop-master hadoop]# source /etc/profile
Metastore
metastore是Hive元数据集中存放地。它包括两部分：服务和后台数据存储。有三种方式配置metastore：内嵌metastore、本地metastore以及远程metastore。
本次搭建中采用MySQL作为远程仓库，部署在hadoop-master节点上，hive服务端也安装在hive-master上，hive客户端即hadoop-slave访问hive服务器。
MySQL安装
安装依赖包
# yum install gcc gcc-c++ ncurses-devel  -y
安装cmake
# wget http://www.cmake.org/files/v2.8/cmake-2.8.12.tar.gz
# tar zxvf cmake-2.8.12.tar.gz
# cd cmake-2.8.12
# ./bootstrap
# make && make install
创建用户的相应目录
# groupadd mysql
# useradd -g mysql mysql
# mkdir -p /data/mysql/
# mkdir -p /data/mysql/data/
# mkdir -p /data/mysql/log/
获取MySQL安装包并安装
# wget http://dev.mysql.com/get/downloads/mysql/mysql-5.6.25.tar.gz
# tar zxvf mysql-5.6.25.tar.gz
# cd mysql-5.6.25
# cmake \
-DCMAKE_INSTALL_PREFIX=/data/mysql \
-DMYSQL_UNIX_ADDR=/data/mysql/mysql.sock \
-DDEFAULT_CHARSET=utf8 \
-DDEFAULT_COLLATION=utf8_general_ci \
-DWITH_INNOBASE_STORAGE_ENGINE=1 \
-DWITH_ARCHIVE_STORAGE_ENGINE=1 \
-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \
-DMYSQL_DATADIR=/data/mysql/data \
-DMYSQL_TCP_PORT=3306 \
-DENABLE_DOWNLOADS=1
如果报错找不到CMakeCache.txt则说明没安装ncurses-devel
# make && make install
修改目录权限
# chmod +w /data/mysql/
# chown -R mysql:mysql /data/mysql/
# ln -s /data/mysql/lib/libmysqlclient.so.18 /usr/lib/libmysqlclient.so.18
# ln -s /data/mysql/mysql.sock /tmp/mysql.sock
初始化数据库
# cp /data/mysql/support-files/my-default.cnf /etc/my.cnf
# cp /data/mysql/support-files/mysql.server /etc/init.d/mysqld
# /data/mysql/scripts/mysql_install_db --user=mysql --defaults-file=/etc/my.cnf --basedir=/data/mysql --datadir=/data/mysql/data
启动MySQL服务
# chmod +x /etc/init.d/mysqld
# service mysqld start
#ln –s /data/mysql/bin/mysql /usr/bin/
初始化密码
#mysql -uroot  -h127.0.0.1 -p
mysql> SET PASSWORD = PASSWORD('123456');
创建Hive用户
mysql>CREATE USER 'hive' IDENTIFIED BY 'hive';
mysql>GRANT ALL PRIVILEGES ON *.* TO 'hive'@'hadoop-master' WITH GRANT OPTION;
mysql>flush privileges;
Hive用户登录
[hadoop@hadoop-master ~]mysql -h hadoop-master -uhive
mysql>set password = password('hive');
创建Hive数据库
mysql>create database hive;
配置Hive
修改配置文件
进入到hive的配置文件目录下，找到hive-default.xml.template，cp份为hive-default.xml
另创建hive-site.xml并添加参数
[hadoop@hadoop-master conf]$ pwd
/home/hadoop/apache-hive-1.2.1-bin/conf
[hadoop@hadoop-master conf]$ vi hive-site.xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hadoop-master:3306/hive?createDatabaseIfNotExist=true</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive<value>
        <description>username to use against metastore database</description>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hive</value>
        <description>password to use against metastore database</description>
    </property>
</configuration>
JDBC下载
[hadoop@hadoop-master ~]$ wget http://cdn.mysql.com/Downloads/Connector-J/mysql-connector-java-5.1.36.tar.gz
[hadoop@hadoop-master ~]$ ls
apache-hive-1.2.1-bin  dfs  hadoop-2.7.1  Hsource  tmp
[hadoop@hadoop-master ~]$ cp mysql-connector-java-5.1.33-bin.jar apache-hive-1.2.1-bin/lib/
Hive客户端配置
[hadoop@hadoop-master ~]$ scp -r apache-hive-1.2.1-bin/ hadoop@hadoop-slave:/home/hadoop
[hadoop@hadoop-slave conf]$ vi hive-site.xml
<configuration>
    <property>
        <name>hive.metastore.uris</name>
    <value>thrift://hadoop-master:9083</value>
    </property>
</configuration>
Hive启动
要启动metastore服务
[hadoop@hadoop-master ~]$ hive --service metastore &
[hadoop@hadoop-master ~]$ jps
10288 RunJar  #多了一个进程
9365 NameNode
9670 SecondaryNameNode
11096 Jps
9944 NodeManager
9838 ResourceManager
9471 DataNode
Hive服务器端访问
[hadoop@hadoop-master ~]$ hive
Logging initialized using configuration in jar:file:/home/hadoop/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive> show databases;
OK
default
src
Time taken: 1.332 seconds, Fetched: 2 row(s)
hive> use src;
OK
Time taken: 0.037 seconds
hive> create table test1(id int);
OK
Time taken: 0.572 seconds
hive> show tables;
OK
abc
test
test1
Time taken: 0.057 seconds, Fetched: 3 row(s)
hive>
Hive客户端访问
[hadoop@hadoop-slave conf]$ hive
Logging initialized using configuration in jar:file:/home/hadoop/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
hive> show databases;
OK
default
src
Time taken: 1.022 seconds, Fetched: 2 row(s)
hive> use src;
OK
Time taken: 0.057 seconds
hive> show tables;
OK
abc
test
test1
Time taken: 0.218 seconds, Fetched: 3 row(s)
hive> create table test2(id int ,name string);
OK
Time taken: 5.518 seconds
hive> show tables;
OK
abc
test
test1
test2
Time taken: 0.102 seconds, Fetched: 4 row(s)
hive>
好了，测试完毕，已经安装成功了。
安装问题纠错
Hive数据库编码问题
错误描述：hive进入后可以创建数据库，但是无法创建表
hive>create table table_test(id string,name string);
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.MetaException(message:javax.jdo.JDODataStoreException: An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes
解决办法：登录mysql修改下hive数据库的编码方式
mysql>alter database hive character set latin1;
防火墙问题
Hive服务器开启了iptables服务，hive本机可以访问hive服务，hive的客户端hadoop-slave访问报错
[hadoop@hadoop-slave conf]$ hive
Logging initialized using configuration in jar:file:/home/hadoop/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties
Exception in thread "main" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
        ... 8 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
        ... 14 more
Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:187)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
        at java.net.Socket.connect(Socket.java:589)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:182)
        ... 22 more
)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:236)
        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
        ... 19 more
解决办法：比较粗暴直接关掉了防火墙
[root@hadoop-master hadoop]# service iptables stop
iptables: Flushing firewall rules: [  OK  ]
iptables: Setting chains to policy ACCEPT: filter [  OK  ]
iptables: Unloading modules: [  OK  ]
[root@hadoop-master hadoop]#
参考资料
    hive元数据库配置Metadata
    Hadoop+Hive环境搭建
    基于Hadoop数据仓库Hive1.2部署及使用


------------------------------------------------------------------------------

Hive的安装
一 安装环境

    Hadoop 2.7.2
    JDK 1.7 U79
    Hive 2.1.0
    Mysql(apt-get 安装)
    192.168.1.166为Mysql server meta server安装位置
    192.168.1.159为Hive数据仓库安装位置

二 Hive的安装-MySQL作为元数据库

    安装JDK-略过
    安装Hadoop-略过
    安装Mysql-略过

三 在192.168.1.166上建立Hive meta数据库,用户,赋予权限

mysql虚拟机的默认密码,在我做试验的时候是123456


$mysql -u root -p
mysql>grant all privileges on *.* to hive@"%" identified by "hive" with grant option;
mysql>flush privileges;
Mysql在Ubuntu中默认安装后,只能在本机访问,如果要开启远程访问,需要做以下两个步骤:
$nano /etc/mysql/my.cnf

找到bind-address=127.0.0.1 ,把这一行注释掉
$service mysql restart

四 在192.168.1.159上安装Hive

1 安装Hive

hadoop@hadoopmaster:~$ sudo tar xvfz apache-hive-2.1.0-bin.tar.gz
hadoop@hadoopmaster:~$ sudo cp -R apache-hive-2.1.0-bin /usr/local/hive
hadoop@hadoopmaster:~$ sudo chmod -R 775 /usr/local/hive/
hadoop@hadoopmaster:~$ sudo chown -R hadoop:hadoop /usr/local/hive

2 修改/etc/profile加入HIVE_HOME的变量

export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib:/usr/local/hive/lib
$source /etc/profile

3 修改hive/conf下的几个template模板并重命名为其他

cp hive-env.sh.template hive-env.sh
cp hive-default.xml.template hive-site.xml

配置hive-env.sh文件，指定HADOOP_HOME

HADOOP_HOME=/usr/local/hadoop

4 修改hive-site.xml文件，指定MySQL数据库驱动、数据库名、用户名及密码，修改的内容如下所示

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://192.168.1.178:3306/hive?createDatabaseIfNotExist=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hive</value>
  <description>username to use against metastore database</description>
</property>
<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hive</value>
  <description>password to use against metastore database</description>
</property>

其中：
javax.jdo.option.ConnectionURL参数指定的是Hive连接数据库的连接字符串；
javax.jdo.option.ConnectionDriverName参数指定的是驱动的类入口名称；
javax.jdo.option.ConnectionUserName参数指定了数据库的用户名；
javax.jdo.option.ConnectionPassword参数指定了数据库的密码。

5 缓存目录的问题,如果不配置也会出错的

 <property>
 <name>hive.exec.local.scratchdir</name>
 <value>/home/hadoop/iotmp</value>
 <description>Local scratch space for Hive jobs</description>
 </property>
 <property>
 <name>hive.downloaded.resources.dir</name>
 <value>/home/hadoop/iotmp</value>
 <description>Temporary local directory for added resources in the remote file system.</description>
 </property>

并且需要对目录进行权限设定

mkdir -p /home/hadoop/iotmp
chmod -R 775 /home/hadoop/iotmp

五修改hive/bin下的hive-config.sh文件，设置JAVA_HOME,HADOOP_HOME

export JAVA_HOME=/usr/lib/jvm
export HADOOP_HOME=/usr/local/hadoop
export HIVE_HOME=/usr/local/hive

六 下载mysql-connector-java-5.1.27-bin.jar文件，并放到$HIVE_HOME/lib目录下

可以从Mysql的官方网站下载,但是记得一定要解压呀,下载的是一个tar.gz文件
七 在HDFS中创建/tmp和/user/hive/warehouse并设置权限

hadoop fs -mkdir /tmp
hadoop fs -mkdir -p /user/hive/warehouse
hadoop fs -chmod g+w /tmp
hadoop fs -chmod g+w /user/hive/warehouse

1 初始化meta数据库

进入之前需要初始化数据库

schematool -initSchema -dbType mysql

hadoop@hadoopmaster:/usr/local/hive/lib$ schematool -initSchema -dbType mysql
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Metastore connection URL: jdbc:mysql://192.168.1.166:3306/hive?createDatabaseIfNotExist=true
Metastore Connection Driver : com.mysql.jdbc.Driver
Metastore connection User: hive
Starting metastore schema initialization to 2.1.0
Initialization script hive-schema-2.1.0.mysql.sql
Initialization script completed
schemaTool completed

2 测试hive shell

hive
show databases;
show tables;

3可以在hadoop中查看hive生产的文件

hadoop dfs -ls /user/hive/warehouse

七 Hive shell使用实例

在正式讲解HiveQL之前,先在命令行下运行几样命令是有好处的,可以感受一下HiveQL是如何工作的,也可以自已随便探索一下.
1 创建数据(文本以tab分隔)

~ vi /home/cos/demo/t_hive.txt

16      2       3
61      12      13
41      2       31
17      21      3
71      2       31
1       12      34
11      2       34

2 创建新表

hive> CREATE TABLE t_hive (a int, b int, c int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
OK
Time taken: 0.121 seconds

3 导入数据t_hive.txt到t_hive表

hive> LOAD DATA LOCAL INPATH '/tmp/t_hive.txt' OVERWRITE INTO TABLE t_hive ;
Loading data to table default.t_hive
OK
Time taken: 0.609 seconds

4 查看表

hive> show tables;
OK
t_hive
Time taken: 0.099 seconds

5 正则匹配表名

hive>show tables '*t*';
OK
t_hive
Time taken: 0.065 seconds

6 查看表数据

hive> select * from t_hive;
OK
16      2       3
61      12      13
41      2       31
17      21      3
71      2       31
1       12      34
11      2       34
Time taken: 0.264 seconds

7 查看表结构

hive> desc t_hive;
OK
a       int
b       int
c       int
Time taken: 0.1 seconds

8 增加一个字段

hive> ALTER TABLE t_hive ADD COLUMNS (new_col String);
OK
Time taken: 0.186 seconds
hive> desc t_hive;
OK
a       int
b       int
c       int
new_col string
Time taken: 0.086 seconds

9 重命令表名

~ ALTER TABLE t_hive RENAME TO t_hadoop;
OK
Time taken: 0.45 seconds
hive> show tables;
OK
t_hadoop
Time taken: 0.07 seconds

10 删除表

hive> DROP TABLE t_hadoop;
OK
Time taken: 0.767 seconds

hive> show tables;
OK
Time taken: 0.064 seconds

八 使用beeline

HiveServer2提供了一个新的命令行工具Beeline，它是基于SQLLine CLI的JDBC客户端。 关于SQLLine的的知识，可以参考这个网站：http://sqlline.sourceforge.net/#manual

Beeline工作模式有两种，即本地嵌入模式和远程模式。嵌入模式情况下，它返回一个嵌入式的Hive（类似于Hive CLI）。而远程模式则是通过Thrift协议与某个单独的HiveServer2进程进行连接通信。下面给一个简单的登录Beeline的使用实例：

1 首先把驱动拷贝到Lib中

sudo cp jdbc/hive-jdbc-2.1.0-standalone.jar /usr/local/hive/lib/

2 启动hiveserver2的服务

命令行模式:

hive --service hiveserver2 --hiveconf hive.server2.thrift.port=10001

服务模式：

hiveserver2 start

3 执行操作

% bin/beeline
Hive version 0.11.0-SNAPSHOT by Apache
beeline> !connect jdbc:hive2://localhost:10000/default
!connect jdbc:hive2://localhost:10000/default
Connecting to jdbc:hive2://localhost:10000/default
Connected to: Hive (version 0.10.0)
Driver: Hive (version 0.10.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://localhost:10000> show tables;
show tables;
+-------------------+
|     tab_name      |
+-------------------+
| primitives        |
| src               |
| src1              |
| src_json          |
| src_sequencefile  |
| src_thrift        |
| srcbucket         |
| srcbucket2        |
| srcpart           |
+-------------------+
9 rows selected (1.079 seconds)

九 FAQ
出错信息1

hadoop@hadoopmaster:/usr/local/hive/conf$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-2.1.0.jar!/hive-log4j2.properties Async: true
Exception in thread "main" java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql))
 at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:578)
 at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:518)
 at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)
 at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql))
 at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:226)
 at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:366)
 at org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:310)
 at org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:290)
 at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:266)
 at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:545)
 ... 9 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql))
 at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3593)
 at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:236)
 at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:221)
 ... 14 more
Caused by: MetaException(message:Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType ...) to create the schema. If needed, don't forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql))
 at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3364)
 at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3336)
 at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3590)
 ... 16 more

没有执行

schematool -initSchema -dbType mysql

执行之后搞定
出错信息2

 hadoop@hadoopmaster:/usr/local/hive/lib$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.4.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/usr/local/hive/lib/hive-common-2.1.0.jar!/hive-log4j2.properties Async: true
Exception in thread "main" java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
 at org.apache.hadoop.fs.Path.initialize(Path.java:205)
 at org.apache.hadoop.fs.Path.<init>(Path.java:171)
 at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:631)
 at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:550)
 at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:518)
 at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:705)
 at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:606)
 at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
 at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: ${system:java.io.tmpdir%7D/$%7Bsystem:user.name%7D
 at java.net.URI.checkPath(URI.java:1804)
 at java.net.URI.<init>(URI.java:752)
 at org.apache.hadoop.fs.Path.initialize(Path.java:202)
 ... 12 more

hive-site.xml中没有配置合理临时目录的问题

 <property>
 <name>hive.exec.local.scratchdir</name>
 <value>/home/hadoop/iotmp</value>
 <description>Local scratch space for Hive jobs</description>
 </property>
 <property>
 <name>hive.downloaded.resources.dir</name>
 <value>/home/hadoop/iotmp</value>
 <description>Temporary local directory for added resources in the remote file system.</description>
 </property>
 <property>

并且需要对目录进行权限设定

mkdir -p /home/hadoop/iotmp
chmod -R 775 /home/hadoop/iotmp

